{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character Level LSTM - PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ThYnzcCIpqx"
      },
      "source": [
        "taken from \n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLoQOxcqhrMx"
      },
      "source": [
        "Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "disFlI5KhIta"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nauslqphh1xZ"
      },
      "source": [
        "Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uQJQUNh3Yk"
      },
      "source": [
        "\n",
        "# open text file and read in data as `text`\n",
        "with open('/content/drive/MyDrive/Sample_txt.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWFV_-Mzh97T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78200a18-b080-4577-8353-44a4ee71bda6"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeffSample Txt for RNN Training\\nby : Palestine Krystal Hill\\n\\n\\nI have been taking a PyTorch course for a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJpmufajiKTF"
      },
      "source": [
        "Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UmDukWiMoF"
      },
      "source": [
        "\n",
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2xgyrxfiUJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558603e9-85fd-4047-fabc-1c3d80151248"
      },
      "source": [
        "encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([38, 22, 36, ...,  1, 40,  6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7QgCJJCibHk"
      },
      "source": [
        "Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzQidjuLid1L"
      },
      "source": [
        "\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYYeCaOijeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588f5085-8c4f-4213-ae4f-233a93030d1c"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smyTUMUti39L"
      },
      "source": [
        "Create batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGXIgbPi74u"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sShslul5jJyK"
      },
      "source": [
        "Test implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhi6dZ1ojM6a"
      },
      "source": [
        "\n",
        "batches = get_batches(encoded, 8, 10)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIf1duaJjOi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418dc8ad-cae0-49d9-a0fc-d90a0af25777"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[38 22 36 21 12 10 11 42 24 15]\n",
            " [40 46 18 16 42 40 11 35 46  1]\n",
            " [16 11 27 42 18 40 42 44  1 42]\n",
            " [42 16 40 31 27 13 42 16 44 21]\n",
            " [ 1 33 42  3 42 46 36 43 11 42]\n",
            " [36 43 11 42 36 42 40 11 15 40]\n",
            " [36  2 11 42 39 31 18 40 11 42]\n",
            " [16 31  2 11 42 27 44 42 46 44]]\n",
            "\n",
            "y\n",
            " [[22 36 21 12 10 11 42 24 15 40]\n",
            " [46 18 16 42 40 11 35 46  1 44]\n",
            " [11 27 42 18 40 42 44  1 42 40]\n",
            " [16 40 31 27 13 42 16 44 21 11]\n",
            " [33 42  3 42 46 36 43 11 42 40]\n",
            " [43 11 42 36 42 40 11 15 40 42]\n",
            " [ 2 11 42 39 31 18 40 11 42 12]\n",
            " [31  2 11 42 27 44 42 46 44 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuExJLmJjc47"
      },
      "source": [
        "LSTM Input output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNzzMk19jgQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d61682-9adf-4e42-8e37-a4df73fa4cce"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU; consider making n_epochs very small.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxse3QbIjrcd"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ_JwgdDj6lv"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REpqusKYj8O8"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRvmcurQkL8C"
      },
      "source": [
        "Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbkLAFIkQFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25eb01b9-0d71-4a93-daea-c7fd4071ace9"
      },
      "source": [
        "\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(47, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=47, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJuZIkMdkXtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8849326-5867-4fcd-f37d-82115f8ce53b"
      },
      "source": [
        "batch_size = 8\n",
        "seq_length = 10\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.1808... Val Loss: 3.3876\n",
            "Epoch: 2/20... Step: 20... Loss: 3.2577... Val Loss: 3.3231\n",
            "Epoch: 3/20... Step: 30... Loss: 3.2616... Val Loss: 3.2355\n",
            "Epoch: 3/20... Step: 40... Loss: 3.1689... Val Loss: 3.2369\n",
            "Epoch: 4/20... Step: 50... Loss: 3.0396... Val Loss: 3.2275\n",
            "Epoch: 5/20... Step: 60... Loss: 3.1922... Val Loss: 3.2172\n",
            "Epoch: 5/20... Step: 70... Loss: 2.9657... Val Loss: 3.2168\n",
            "Epoch: 6/20... Step: 80... Loss: 3.0302... Val Loss: 3.2169\n",
            "Epoch: 7/20... Step: 90... Loss: 3.1307... Val Loss: 3.2139\n",
            "Epoch: 8/20... Step: 100... Loss: 3.1929... Val Loss: 3.2106\n",
            "Epoch: 8/20... Step: 110... Loss: 3.1075... Val Loss: 3.2113\n",
            "Epoch: 9/20... Step: 120... Loss: 2.9610... Val Loss: 3.1879\n",
            "Epoch: 10/20... Step: 130... Loss: 3.1324... Val Loss: 3.1783\n",
            "Epoch: 10/20... Step: 140... Loss: 2.9149... Val Loss: 3.1727\n",
            "Epoch: 11/20... Step: 150... Loss: 2.9513... Val Loss: 3.1358\n",
            "Epoch: 12/20... Step: 160... Loss: 2.9492... Val Loss: 3.1237\n",
            "Epoch: 13/20... Step: 170... Loss: 2.9790... Val Loss: 3.0819\n",
            "Epoch: 13/20... Step: 180... Loss: 2.8830... Val Loss: 3.0611\n",
            "Epoch: 14/20... Step: 190... Loss: 2.7127... Val Loss: 2.9924\n",
            "Epoch: 15/20... Step: 200... Loss: 2.8924... Val Loss: 2.9849\n",
            "Epoch: 15/20... Step: 210... Loss: 2.6369... Val Loss: 3.0240\n",
            "Epoch: 16/20... Step: 220... Loss: 2.6111... Val Loss: 2.9250\n",
            "Epoch: 17/20... Step: 230... Loss: 2.5356... Val Loss: 2.9413\n",
            "Epoch: 18/20... Step: 240... Loss: 2.6732... Val Loss: 2.9273\n",
            "Epoch: 18/20... Step: 250... Loss: 2.6326... Val Loss: 2.9234\n",
            "Epoch: 19/20... Step: 260... Loss: 2.4638... Val Loss: 2.8909\n",
            "Epoch: 20/20... Step: 270... Loss: 2.5111... Val Loss: 2.8367\n",
            "Epoch: 20/20... Step: 280... Loss: 2.3949... Val Loss: 2.8442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqbiCeIfkgXq"
      },
      "source": [
        "Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ildCY3kkBf"
      },
      "source": [
        "\n",
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnXPXfBHkvX9"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kq-UHEgku8m"
      },
      "source": [
        "\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoFZ8Hhjk7NI"
      },
      "source": [
        "Prime & generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0VPhoITk-ti"
      },
      "source": [
        "\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ55LvF8lELv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e995596f-cb3d-4e7f-d6ec-33d3ced65dc8"
      },
      "source": [
        "print(sample(net, 1000, prime='Sample', top_k=5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample oon toute sote meite t aoettnend toral tane boe se saredn an wrasetiiinn oaveeee tiie shat we nave toute tecsrlite to hol theite me the wr sne wo tiee ane teusilit te move tet ooe toeetn se thee tit teentite ao taee te tite morin t thave teen aon an aheet need to ho tam tee mo masetta mite sotet male to tevegsinnd oe areen noe te tave teete the heet meed teite t vale te te mat te tive aieiie de save soine t ahee tale taledn toure to roed ne wave moteen d ch sereiie te to tavee to hhe t ae nire teureed no te torite nitane te hovettee meite woctat te meine ao hive ahe sote ooe socas taee, noe te hase toentg te haveetoiite to aave notite te shed taiee naty tot mectettee ao we socrile totett nite hoe aneente tee toeesen nouhae toet neil aoednn aaeeteent tocuteente nave te teutet oee sout ao tee tale to mate setise save aale met tocsstite tavilit notatotite te motenite oocus ne te hate teiiil at tove saenng oacatet ine tave taee te seure theddane so sase noutane ohasette toily ae srtetine t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSsTb1FmlJyW"
      },
      "source": [
        "Load a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk70l_uKlMOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21eaec1-f961-4b13-e594-ec6aeed0816d"
      },
      "source": [
        "\n",
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJ47OE1lQa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebe0835-ef28-4ca7-fbd4-0e8c52e0364a"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"I have been\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have beentaitin taat teit te tee mavee toest me tatettee nat aorentine toal the wr metile aoile te wred tote tile beare moty tat wile te teite mo te havedennt hoc haee aeednn ooaure teeteed nod te thile mein te sore tine aaedtite secs he seseeedd ne have aeedten necwostne touree aoute te hoe shet torsine aa aoe ne tersete te tasedine aa sheito no tave me tot wet teute naiinet t aae seed tile oh toved totite ahe toe te tettite to he terrrint tecrole ne tave t cainend te hhey hoite toit tacontent tove toure toume te sare maieite the araeed iine teasetet no hev teenenin caatetitee soute soche torsten aaresioo tousengnn tave to mole,, oo she teinine\n",
            " noctesit iite te tave, aee te te sore te tat te mhile no sare teined s coutoee tacs so totinn aeute to thisy an toreeen nn ao have ne hoe morett oave te tave meten tac toveenn in teuhee toetne touse tote mocs hoc aritee oo mee te masesto tive, oee thed hocsanee, aoise on tave matet toite t aoe thet aaie te whe hociree toiten Iote mee titn tochele toule t tatetoiee ne have,teitee, aousedtt have, aeet onule t matedeite t te waretiiin do aale te moteet, to waveet noitn thovedt tain dh hat thite seised seute he we te teuret te mite sotee me misn dacaletinnn\n",
            " oaaeedntedon t m tove ie techet ao ne te weestite an wave toeet oe he horeed tanet oore itit teeseite ne tret ieeed ae teitae tet aoit taved nat te heve ton the se seisen toasite te tave toet toeyt oave, teit te totete taite, tae toee tee oo te tave teitee te we motet no wored thidy oe weite touned so sate mee nne, to sav tiiiiin toareeneini oot te hevette tede bove tave te te movye,,, tocs toe shet are to masn sethoe nate te tavy to shil ae metiine teourie tothite aoetnoten ne haeediine toutited te set torsene taty toe te tave nedite boe areen toit satitoe tite sorine toutet tive tat meet teednoro tee tale noteitid cochave tee te teitail to sove aae tee toite ao to mectette seure toe toredtote te tresnotn tous tal no matetteniin thhatetteity totertett toctesrine to ho t to tetdetn t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character Level LSTM - PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ThYnzcCIpqx"
      },
      "source": [
        "taken from \n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoQWQA8jrYnZ"
      },
      "source": [
        "text can be found at:-\n",
        "https://raw.githubusercontent.com/WillKoehrsen/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLoQOxcqhrMx"
      },
      "source": [
        "Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "disFlI5KhIta"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nauslqphh1xZ"
      },
      "source": [
        "Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uQJQUNh3Yk"
      },
      "source": [
        "\n",
        "# open text file and read in data as `text`\n",
        "with open('/content/drive/MyDrive/AnnaK.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWFV_-Mzh97T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6386cea-2bfe-4268-c5d8-9c311daa51ee"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJpmufajiKTF"
      },
      "source": [
        "Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UmDukWiMoF"
      },
      "source": [
        "\n",
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2xgyrxfiUJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd2ec46-cf16-4b2d-b5da-055e296ab7c6"
      },
      "source": [
        "encoded"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 75, 35, ...,  0, 76, 76])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7QgCJJCibHk"
      },
      "source": [
        "Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzQidjuLid1L"
      },
      "source": [
        "\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYYeCaOijeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b74a44-4452-4514-e440-1dd1fd3391c0"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smyTUMUti39L"
      },
      "source": [
        "Create batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGXIgbPi74u"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sShslul5jJyK"
      },
      "source": [
        "Test implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhi6dZ1ojM6a"
      },
      "source": [
        "\n",
        "batches = get_batches(encoded, 64, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIf1duaJjOi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfda01e7-12cd-46de-dbb8-2a4f553fe375"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[16 75 35 33 61  3 37 25 20 76]\n",
            " [48 25 61 78 25 33 57 61 25 78]\n",
            " [48 78 34 26 61 73 75 79 25  5]\n",
            " [48  3 68  3 17 72 25 57 48  5]\n",
            " [68 79 25  5 75 78 76 35 17  5]\n",
            " [46 46 26 48 68 61 35 48 61 35]\n",
            " [61 70 26 48 28 25  5 26 61 75]\n",
            " [76 55 48 11 25 75  3 25 52  3]\n",
            " [11 25 35 48 11 25 78 72 25 68]\n",
            " [25 68 61 26 17 17 76 71 78 57]]\n",
            "\n",
            "y\n",
            " [[75 35 33 61  3 37 25 20 76 76]\n",
            " [25 61 78 25 33 57 61 25 78 48]\n",
            " [78 34 26 61 73 75 79 25  5 75]\n",
            " [ 3 68  3 17 72 25 57 48  5 78]\n",
            " [79 25  5 75 78 76 35 17  5 35]\n",
            " [46 26 48 68 61 35 48 61 35 48]\n",
            " [70 26 48 28 25  5 26 61 75 25]\n",
            " [55 48 11 25 75  3 25 52  3 28]\n",
            " [25 35 48 11 25 78 72 25 68 78]\n",
            " [68 61 26 17 17 76 71 78 57 48]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuExJLmJjc47"
      },
      "source": [
        "LSTM Input output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNzzMk19jgQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65ccd89-09d8-4083-9029-937b20346db4"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxse3QbIjrcd"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ_JwgdDj6lv"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REpqusKYj8O8"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRvmcurQkL8C"
      },
      "source": [
        "Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbkLAFIkQFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3721b964-68b4-4376-c7ac-1c15dd393f3c"
      },
      "source": [
        "\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJuZIkMdkXtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28ada7a-ab5c-4677-9384-776ef7f8166e"
      },
      "source": [
        "batch_size = 64\n",
        "seq_length = 80\n",
        "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.2384... Val Loss: 3.2225\n",
            "Epoch: 1/50... Step: 20... Loss: 3.1414... Val Loss: 3.1405\n",
            "Epoch: 1/50... Step: 30... Loss: 3.1349... Val Loss: 3.1280\n",
            "Epoch: 1/50... Step: 40... Loss: 3.1382... Val Loss: 3.1214\n",
            "Epoch: 1/50... Step: 50... Loss: 3.1060... Val Loss: 3.1207\n",
            "Epoch: 1/50... Step: 60... Loss: 3.1173... Val Loss: 3.1178\n",
            "Epoch: 1/50... Step: 70... Loss: 3.1212... Val Loss: 3.1157\n",
            "Epoch: 1/50... Step: 80... Loss: 3.1045... Val Loss: 3.1107\n",
            "Epoch: 1/50... Step: 90... Loss: 3.1098... Val Loss: 3.1013\n",
            "Epoch: 1/50... Step: 100... Loss: 3.1029... Val Loss: 3.0895\n",
            "Epoch: 1/50... Step: 110... Loss: 3.0546... Val Loss: 3.0426\n",
            "Epoch: 1/50... Step: 120... Loss: 2.9636... Val Loss: 2.9465\n",
            "Epoch: 1/50... Step: 130... Loss: 2.8486... Val Loss: 2.8192\n",
            "Epoch: 1/50... Step: 140... Loss: 2.8102... Val Loss: 2.7699\n",
            "Epoch: 1/50... Step: 150... Loss: 2.6545... Val Loss: 2.6463\n",
            "Epoch: 1/50... Step: 160... Loss: 2.5839... Val Loss: 2.5711\n",
            "Epoch: 1/50... Step: 170... Loss: 2.5238... Val Loss: 2.5219\n",
            "Epoch: 1/50... Step: 180... Loss: 2.5115... Val Loss: 2.4821\n",
            "Epoch: 1/50... Step: 190... Loss: 2.4694... Val Loss: 2.4439\n",
            "Epoch: 1/50... Step: 200... Loss: 2.4590... Val Loss: 2.4120\n",
            "Epoch: 1/50... Step: 210... Loss: 2.3923... Val Loss: 2.3904\n",
            "Epoch: 1/50... Step: 220... Loss: 2.3957... Val Loss: 2.3646\n",
            "Epoch: 1/50... Step: 230... Loss: 2.3545... Val Loss: 2.3393\n",
            "Epoch: 1/50... Step: 240... Loss: 2.2871... Val Loss: 2.3180\n",
            "Epoch: 1/50... Step: 250... Loss: 2.2906... Val Loss: 2.2899\n",
            "Epoch: 1/50... Step: 260... Loss: 2.2641... Val Loss: 2.2697\n",
            "Epoch: 1/50... Step: 270... Loss: 2.2757... Val Loss: 2.2549\n",
            "Epoch: 1/50... Step: 280... Loss: 2.2347... Val Loss: 2.2376\n",
            "Epoch: 1/50... Step: 290... Loss: 2.1931... Val Loss: 2.2095\n",
            "Epoch: 1/50... Step: 300... Loss: 2.1728... Val Loss: 2.1879\n",
            "Epoch: 1/50... Step: 310... Loss: 2.1692... Val Loss: 2.1673\n",
            "Epoch: 1/50... Step: 320... Loss: 2.1383... Val Loss: 2.1601\n",
            "Epoch: 1/50... Step: 330... Loss: 2.1322... Val Loss: 2.1302\n",
            "Epoch: 1/50... Step: 340... Loss: 2.1427... Val Loss: 2.1130\n",
            "Epoch: 2/50... Step: 350... Loss: 2.1352... Val Loss: 2.0947\n",
            "Epoch: 2/50... Step: 360... Loss: 2.0958... Val Loss: 2.0883\n",
            "Epoch: 2/50... Step: 370... Loss: 2.0790... Val Loss: 2.0689\n",
            "Epoch: 2/50... Step: 380... Loss: 2.0210... Val Loss: 2.0593\n",
            "Epoch: 2/50... Step: 390... Loss: 2.0443... Val Loss: 2.0396\n",
            "Epoch: 2/50... Step: 400... Loss: 2.0013... Val Loss: 2.0250\n",
            "Epoch: 2/50... Step: 410... Loss: 1.9969... Val Loss: 2.0162\n",
            "Epoch: 2/50... Step: 420... Loss: 1.9855... Val Loss: 1.9945\n",
            "Epoch: 2/50... Step: 430... Loss: 2.0178... Val Loss: 1.9832\n",
            "Epoch: 2/50... Step: 440... Loss: 2.0561... Val Loss: 1.9693\n",
            "Epoch: 2/50... Step: 450... Loss: 1.9777... Val Loss: 1.9568\n",
            "Epoch: 2/50... Step: 460... Loss: 1.9887... Val Loss: 1.9512\n",
            "Epoch: 2/50... Step: 470... Loss: 1.9679... Val Loss: 1.9358\n",
            "Epoch: 2/50... Step: 480... Loss: 1.9207... Val Loss: 1.9202\n",
            "Epoch: 2/50... Step: 490... Loss: 1.8871... Val Loss: 1.9142\n",
            "Epoch: 2/50... Step: 500... Loss: 1.8901... Val Loss: 1.9013\n",
            "Epoch: 2/50... Step: 510... Loss: 1.8643... Val Loss: 1.8884\n",
            "Epoch: 2/50... Step: 520... Loss: 1.8438... Val Loss: 1.8849\n",
            "Epoch: 2/50... Step: 530... Loss: 1.9355... Val Loss: 1.8702\n",
            "Epoch: 2/50... Step: 540... Loss: 1.8732... Val Loss: 1.8620\n",
            "Epoch: 2/50... Step: 550... Loss: 1.8488... Val Loss: 1.8497\n",
            "Epoch: 2/50... Step: 560... Loss: 1.8633... Val Loss: 1.8420\n",
            "Epoch: 2/50... Step: 570... Loss: 1.8159... Val Loss: 1.8406\n",
            "Epoch: 2/50... Step: 580... Loss: 1.7880... Val Loss: 1.8292\n",
            "Epoch: 2/50... Step: 590... Loss: 1.8031... Val Loss: 1.8211\n",
            "Epoch: 2/50... Step: 600... Loss: 1.8268... Val Loss: 1.8114\n",
            "Epoch: 2/50... Step: 610... Loss: 1.8409... Val Loss: 1.8058\n",
            "Epoch: 2/50... Step: 620... Loss: 1.8054... Val Loss: 1.7967\n",
            "Epoch: 2/50... Step: 630... Loss: 1.8044... Val Loss: 1.7865\n",
            "Epoch: 2/50... Step: 640... Loss: 1.8043... Val Loss: 1.7855\n",
            "Epoch: 2/50... Step: 650... Loss: 1.7467... Val Loss: 1.7694\n",
            "Epoch: 2/50... Step: 660... Loss: 1.7660... Val Loss: 1.7681\n",
            "Epoch: 2/50... Step: 670... Loss: 1.7980... Val Loss: 1.7582\n",
            "Epoch: 2/50... Step: 680... Loss: 1.8420... Val Loss: 1.7502\n",
            "Epoch: 2/50... Step: 690... Loss: 1.7412... Val Loss: 1.7408\n",
            "Epoch: 3/50... Step: 700... Loss: 1.7086... Val Loss: 1.7414\n",
            "Epoch: 3/50... Step: 710... Loss: 1.7554... Val Loss: 1.7280\n",
            "Epoch: 3/50... Step: 720... Loss: 1.6986... Val Loss: 1.7259\n",
            "Epoch: 3/50... Step: 730... Loss: 1.6769... Val Loss: 1.7173\n",
            "Epoch: 3/50... Step: 740... Loss: 1.7343... Val Loss: 1.7132\n",
            "Epoch: 3/50... Step: 750... Loss: 1.7001... Val Loss: 1.7058\n",
            "Epoch: 3/50... Step: 760... Loss: 1.7326... Val Loss: 1.6984\n",
            "Epoch: 3/50... Step: 770... Loss: 1.6967... Val Loss: 1.6938\n",
            "Epoch: 3/50... Step: 780... Loss: 1.6599... Val Loss: 1.6862\n",
            "Epoch: 3/50... Step: 790... Loss: 1.6521... Val Loss: 1.6820\n",
            "Epoch: 3/50... Step: 800... Loss: 1.6825... Val Loss: 1.6777\n",
            "Epoch: 3/50... Step: 810... Loss: 1.6595... Val Loss: 1.6742\n",
            "Epoch: 3/50... Step: 820... Loss: 1.6380... Val Loss: 1.6676\n",
            "Epoch: 3/50... Step: 830... Loss: 1.6828... Val Loss: 1.6687\n",
            "Epoch: 3/50... Step: 840... Loss: 1.6629... Val Loss: 1.6626\n",
            "Epoch: 3/50... Step: 850... Loss: 1.6375... Val Loss: 1.6513\n",
            "Epoch: 3/50... Step: 860... Loss: 1.6688... Val Loss: 1.6455\n",
            "Epoch: 3/50... Step: 870... Loss: 1.6297... Val Loss: 1.6435\n",
            "Epoch: 3/50... Step: 880... Loss: 1.6441... Val Loss: 1.6417\n",
            "Epoch: 3/50... Step: 890... Loss: 1.6556... Val Loss: 1.6437\n",
            "Epoch: 3/50... Step: 900... Loss: 1.5759... Val Loss: 1.6317\n",
            "Epoch: 3/50... Step: 910... Loss: 1.5911... Val Loss: 1.6221\n",
            "Epoch: 3/50... Step: 920... Loss: 1.6187... Val Loss: 1.6246\n",
            "Epoch: 3/50... Step: 930... Loss: 1.5774... Val Loss: 1.6201\n",
            "Epoch: 3/50... Step: 940... Loss: 1.6550... Val Loss: 1.6219\n",
            "Epoch: 3/50... Step: 950... Loss: 1.6233... Val Loss: 1.6202\n",
            "Epoch: 3/50... Step: 960... Loss: 1.6311... Val Loss: 1.6103\n",
            "Epoch: 3/50... Step: 970... Loss: 1.6176... Val Loss: 1.6040\n",
            "Epoch: 3/50... Step: 980... Loss: 1.5938... Val Loss: 1.5988\n",
            "Epoch: 3/50... Step: 990... Loss: 1.5848... Val Loss: 1.5932\n",
            "Epoch: 3/50... Step: 1000... Loss: 1.6216... Val Loss: 1.5943\n",
            "Epoch: 3/50... Step: 1010... Loss: 1.5959... Val Loss: 1.5958\n",
            "Epoch: 3/50... Step: 1020... Loss: 1.6052... Val Loss: 1.5855\n",
            "Epoch: 3/50... Step: 1030... Loss: 1.6492... Val Loss: 1.5804\n",
            "Epoch: 3/50... Step: 1040... Loss: 1.5580... Val Loss: 1.5773\n",
            "Epoch: 4/50... Step: 1050... Loss: 1.5703... Val Loss: 1.5802\n",
            "Epoch: 4/50... Step: 1060... Loss: 1.5512... Val Loss: 1.5773\n",
            "Epoch: 4/50... Step: 1070... Loss: 1.6160... Val Loss: 1.5718\n",
            "Epoch: 4/50... Step: 1080... Loss: 1.5864... Val Loss: 1.5653\n",
            "Epoch: 4/50... Step: 1090... Loss: 1.5865... Val Loss: 1.5538\n",
            "Epoch: 4/50... Step: 1100... Loss: 1.5666... Val Loss: 1.5522\n",
            "Epoch: 4/50... Step: 1110... Loss: 1.5739... Val Loss: 1.5554\n",
            "Epoch: 4/50... Step: 1120... Loss: 1.5811... Val Loss: 1.5505\n",
            "Epoch: 4/50... Step: 1130... Loss: 1.4924... Val Loss: 1.5514\n",
            "Epoch: 4/50... Step: 1140... Loss: 1.5548... Val Loss: 1.5507\n",
            "Epoch: 4/50... Step: 1150... Loss: 1.4944... Val Loss: 1.5468\n",
            "Epoch: 4/50... Step: 1160... Loss: 1.4890... Val Loss: 1.5433\n",
            "Epoch: 4/50... Step: 1170... Loss: 1.5091... Val Loss: 1.5418\n",
            "Epoch: 4/50... Step: 1180... Loss: 1.5626... Val Loss: 1.5382\n",
            "Epoch: 4/50... Step: 1190... Loss: 1.4812... Val Loss: 1.5339\n",
            "Epoch: 4/50... Step: 1200... Loss: 1.5240... Val Loss: 1.5293\n",
            "Epoch: 4/50... Step: 1210... Loss: 1.5366... Val Loss: 1.5294\n",
            "Epoch: 4/50... Step: 1220... Loss: 1.5380... Val Loss: 1.5255\n",
            "Epoch: 4/50... Step: 1230... Loss: 1.5370... Val Loss: 1.5230\n",
            "Epoch: 4/50... Step: 1240... Loss: 1.5292... Val Loss: 1.5190\n",
            "Epoch: 4/50... Step: 1250... Loss: 1.4809... Val Loss: 1.5157\n",
            "Epoch: 4/50... Step: 1260... Loss: 1.5067... Val Loss: 1.5165\n",
            "Epoch: 4/50... Step: 1270... Loss: 1.5319... Val Loss: 1.5194\n",
            "Epoch: 4/50... Step: 1280... Loss: 1.5386... Val Loss: 1.5190\n",
            "Epoch: 4/50... Step: 1290... Loss: 1.5247... Val Loss: 1.5160\n",
            "Epoch: 4/50... Step: 1300... Loss: 1.4855... Val Loss: 1.5106\n",
            "Epoch: 4/50... Step: 1310... Loss: 1.5135... Val Loss: 1.5052\n",
            "Epoch: 4/50... Step: 1320... Loss: 1.5030... Val Loss: 1.5044\n",
            "Epoch: 4/50... Step: 1330... Loss: 1.4952... Val Loss: 1.5015\n",
            "Epoch: 4/50... Step: 1340... Loss: 1.4630... Val Loss: 1.4969\n",
            "Epoch: 4/50... Step: 1350... Loss: 1.4755... Val Loss: 1.5003\n",
            "Epoch: 4/50... Step: 1360... Loss: 1.4794... Val Loss: 1.5007\n",
            "Epoch: 4/50... Step: 1370... Loss: 1.5037... Val Loss: 1.4964\n",
            "Epoch: 4/50... Step: 1380... Loss: 1.5271... Val Loss: 1.4946\n",
            "Epoch: 4/50... Step: 1390... Loss: 1.4630... Val Loss: 1.4861\n",
            "Epoch: 5/50... Step: 1400... Loss: 1.4746... Val Loss: 1.4955\n",
            "Epoch: 5/50... Step: 1410... Loss: 1.4795... Val Loss: 1.4938\n",
            "Epoch: 5/50... Step: 1420... Loss: 1.4804... Val Loss: 1.4835\n",
            "Epoch: 5/50... Step: 1430... Loss: 1.5000... Val Loss: 1.4794\n",
            "Epoch: 5/50... Step: 1440... Loss: 1.4809... Val Loss: 1.4762\n",
            "Epoch: 5/50... Step: 1450... Loss: 1.4668... Val Loss: 1.4752\n",
            "Epoch: 5/50... Step: 1460... Loss: 1.3967... Val Loss: 1.4715\n",
            "Epoch: 5/50... Step: 1470... Loss: 1.4570... Val Loss: 1.4718\n",
            "Epoch: 5/50... Step: 1480... Loss: 1.4445... Val Loss: 1.4777\n",
            "Epoch: 5/50... Step: 1490... Loss: 1.4644... Val Loss: 1.4724\n",
            "Epoch: 5/50... Step: 1500... Loss: 1.4540... Val Loss: 1.4664\n",
            "Epoch: 5/50... Step: 1510... Loss: 1.4911... Val Loss: 1.4655\n",
            "Epoch: 5/50... Step: 1520... Loss: 1.4120... Val Loss: 1.4674\n",
            "Epoch: 5/50... Step: 1530... Loss: 1.4575... Val Loss: 1.4600\n",
            "Epoch: 5/50... Step: 1540... Loss: 1.4056... Val Loss: 1.4606\n",
            "Epoch: 5/50... Step: 1550... Loss: 1.4308... Val Loss: 1.4559\n",
            "Epoch: 5/50... Step: 1560... Loss: 1.4486... Val Loss: 1.4561\n",
            "Epoch: 5/50... Step: 1570... Loss: 1.4313... Val Loss: 1.4496\n",
            "Epoch: 5/50... Step: 1580... Loss: 1.4124... Val Loss: 1.4529\n",
            "Epoch: 5/50... Step: 1590... Loss: 1.3962... Val Loss: 1.4525\n",
            "Epoch: 5/50... Step: 1600... Loss: 1.4394... Val Loss: 1.4517\n",
            "Epoch: 5/50... Step: 1610... Loss: 1.4624... Val Loss: 1.4534\n",
            "Epoch: 5/50... Step: 1620... Loss: 1.4658... Val Loss: 1.4494\n",
            "Epoch: 5/50... Step: 1630... Loss: 1.4455... Val Loss: 1.4500\n",
            "Epoch: 5/50... Step: 1640... Loss: 1.4513... Val Loss: 1.4475\n",
            "Epoch: 5/50... Step: 1650... Loss: 1.4582... Val Loss: 1.4439\n",
            "Epoch: 5/50... Step: 1660... Loss: 1.4588... Val Loss: 1.4443\n",
            "Epoch: 5/50... Step: 1670... Loss: 1.3657... Val Loss: 1.4422\n",
            "Epoch: 5/50... Step: 1680... Loss: 1.4820... Val Loss: 1.4435\n",
            "Epoch: 5/50... Step: 1690... Loss: 1.4488... Val Loss: 1.4391\n",
            "Epoch: 5/50... Step: 1700... Loss: 1.4013... Val Loss: 1.4417\n",
            "Epoch: 5/50... Step: 1710... Loss: 1.4061... Val Loss: 1.4417\n",
            "Epoch: 5/50... Step: 1720... Loss: 1.4426... Val Loss: 1.4372\n",
            "Epoch: 5/50... Step: 1730... Loss: 1.4424... Val Loss: 1.4320\n",
            "Epoch: 5/50... Step: 1740... Loss: 1.4481... Val Loss: 1.4277\n",
            "Epoch: 6/50... Step: 1750... Loss: 1.4054... Val Loss: 1.4312\n",
            "Epoch: 6/50... Step: 1760... Loss: 1.4296... Val Loss: 1.4281\n",
            "Epoch: 6/50... Step: 1770... Loss: 1.4273... Val Loss: 1.4274\n",
            "Epoch: 6/50... Step: 1780... Loss: 1.3979... Val Loss: 1.4262\n",
            "Epoch: 6/50... Step: 1790... Loss: 1.3904... Val Loss: 1.4258\n",
            "Epoch: 6/50... Step: 1800... Loss: 1.4244... Val Loss: 1.4267\n",
            "Epoch: 6/50... Step: 1810... Loss: 1.3860... Val Loss: 1.4224\n",
            "Epoch: 6/50... Step: 1820... Loss: 1.3871... Val Loss: 1.4238\n",
            "Epoch: 6/50... Step: 1830... Loss: 1.3801... Val Loss: 1.4259\n",
            "Epoch: 6/50... Step: 1840... Loss: 1.3809... Val Loss: 1.4275\n",
            "Epoch: 6/50... Step: 1850... Loss: 1.4063... Val Loss: 1.4199\n",
            "Epoch: 6/50... Step: 1860... Loss: 1.3757... Val Loss: 1.4179\n",
            "Epoch: 6/50... Step: 1870... Loss: 1.4148... Val Loss: 1.4212\n",
            "Epoch: 6/50... Step: 1880... Loss: 1.3584... Val Loss: 1.4201\n",
            "Epoch: 6/50... Step: 1890... Loss: 1.3469... Val Loss: 1.4159\n",
            "Epoch: 6/50... Step: 1900... Loss: 1.4039... Val Loss: 1.4147\n",
            "Epoch: 6/50... Step: 1910... Loss: 1.3364... Val Loss: 1.4110\n",
            "Epoch: 6/50... Step: 1920... Loss: 1.3513... Val Loss: 1.4056\n",
            "Epoch: 6/50... Step: 1930... Loss: 1.4427... Val Loss: 1.4071\n",
            "Epoch: 6/50... Step: 1940... Loss: 1.4152... Val Loss: 1.4068\n",
            "Epoch: 6/50... Step: 1950... Loss: 1.3663... Val Loss: 1.4073\n",
            "Epoch: 6/50... Step: 1960... Loss: 1.4565... Val Loss: 1.4055\n",
            "Epoch: 6/50... Step: 1970... Loss: 1.3836... Val Loss: 1.4042\n",
            "Epoch: 6/50... Step: 1980... Loss: 1.3502... Val Loss: 1.4055\n",
            "Epoch: 6/50... Step: 1990... Loss: 1.3433... Val Loss: 1.4055\n",
            "Epoch: 6/50... Step: 2000... Loss: 1.3558... Val Loss: 1.3998\n",
            "Epoch: 6/50... Step: 2010... Loss: 1.3728... Val Loss: 1.3987\n",
            "Epoch: 6/50... Step: 2020... Loss: 1.3816... Val Loss: 1.4003\n",
            "Epoch: 6/50... Step: 2030... Loss: 1.3389... Val Loss: 1.4000\n",
            "Epoch: 6/50... Step: 2040... Loss: 1.3634... Val Loss: 1.3983\n",
            "Epoch: 6/50... Step: 2050... Loss: 1.3961... Val Loss: 1.4022\n",
            "Epoch: 6/50... Step: 2060... Loss: 1.3894... Val Loss: 1.4022\n",
            "Epoch: 6/50... Step: 2070... Loss: 1.3409... Val Loss: 1.3979\n",
            "Epoch: 6/50... Step: 2080... Loss: 1.3819... Val Loss: 1.3908\n",
            "Epoch: 7/50... Step: 2090... Loss: 1.3917... Val Loss: 1.3887\n",
            "Epoch: 7/50... Step: 2100... Loss: 1.3568... Val Loss: 1.3939\n",
            "Epoch: 7/50... Step: 2110... Loss: 1.3805... Val Loss: 1.3905\n",
            "Epoch: 7/50... Step: 2120... Loss: 1.3576... Val Loss: 1.3947\n",
            "Epoch: 7/50... Step: 2130... Loss: 1.3453... Val Loss: 1.3884\n",
            "Epoch: 7/50... Step: 2140... Loss: 1.2993... Val Loss: 1.3855\n",
            "Epoch: 7/50... Step: 2150... Loss: 1.3202... Val Loss: 1.3884\n",
            "Epoch: 7/50... Step: 2160... Loss: 1.2931... Val Loss: 1.3861\n",
            "Epoch: 7/50... Step: 2170... Loss: 1.3571... Val Loss: 1.3818\n",
            "Epoch: 7/50... Step: 2180... Loss: 1.3902... Val Loss: 1.3857\n",
            "Epoch: 7/50... Step: 2190... Loss: 1.3216... Val Loss: 1.3867\n",
            "Epoch: 7/50... Step: 2200... Loss: 1.3523... Val Loss: 1.3827\n",
            "Epoch: 7/50... Step: 2210... Loss: 1.3381... Val Loss: 1.3822\n",
            "Epoch: 7/50... Step: 2220... Loss: 1.2711... Val Loss: 1.3824\n",
            "Epoch: 7/50... Step: 2230... Loss: 1.2874... Val Loss: 1.3827\n",
            "Epoch: 7/50... Step: 2240... Loss: 1.3108... Val Loss: 1.3787\n",
            "Epoch: 7/50... Step: 2250... Loss: 1.3027... Val Loss: 1.3767\n",
            "Epoch: 7/50... Step: 2260... Loss: 1.3130... Val Loss: 1.3765\n",
            "Epoch: 7/50... Step: 2270... Loss: 1.3518... Val Loss: 1.3755\n",
            "Epoch: 7/50... Step: 2280... Loss: 1.3513... Val Loss: 1.3771\n",
            "Epoch: 7/50... Step: 2290... Loss: 1.3119... Val Loss: 1.3749\n",
            "Epoch: 7/50... Step: 2300... Loss: 1.3465... Val Loss: 1.3726\n",
            "Epoch: 7/50... Step: 2310... Loss: 1.3321... Val Loss: 1.3713\n",
            "Epoch: 7/50... Step: 2320... Loss: 1.3174... Val Loss: 1.3696\n",
            "Epoch: 7/50... Step: 2330... Loss: 1.3066... Val Loss: 1.3710\n",
            "Epoch: 7/50... Step: 2340... Loss: 1.3229... Val Loss: 1.3710\n",
            "Epoch: 7/50... Step: 2350... Loss: 1.3635... Val Loss: 1.3664\n",
            "Epoch: 7/50... Step: 2360... Loss: 1.3489... Val Loss: 1.3687\n",
            "Epoch: 7/50... Step: 2370... Loss: 1.3263... Val Loss: 1.3686\n",
            "Epoch: 7/50... Step: 2380... Loss: 1.3299... Val Loss: 1.3652\n",
            "Epoch: 7/50... Step: 2390... Loss: 1.3027... Val Loss: 1.3652\n",
            "Epoch: 7/50... Step: 2400... Loss: 1.3121... Val Loss: 1.3656\n",
            "Epoch: 7/50... Step: 2410... Loss: 1.3548... Val Loss: 1.3646\n",
            "Epoch: 7/50... Step: 2420... Loss: 1.3795... Val Loss: 1.3602\n",
            "Epoch: 7/50... Step: 2430... Loss: 1.3004... Val Loss: 1.3605\n",
            "Epoch: 8/50... Step: 2440... Loss: 1.3056... Val Loss: 1.3604\n",
            "Epoch: 8/50... Step: 2450... Loss: 1.3566... Val Loss: 1.3625\n",
            "Epoch: 8/50... Step: 2460... Loss: 1.2857... Val Loss: 1.3596\n",
            "Epoch: 8/50... Step: 2470... Loss: 1.2784... Val Loss: 1.3616\n",
            "Epoch: 8/50... Step: 2480... Loss: 1.3171... Val Loss: 1.3561\n",
            "Epoch: 8/50... Step: 2490... Loss: 1.2955... Val Loss: 1.3538\n",
            "Epoch: 8/50... Step: 2500... Loss: 1.3260... Val Loss: 1.3540\n",
            "Epoch: 8/50... Step: 2510... Loss: 1.3259... Val Loss: 1.3575\n",
            "Epoch: 8/50... Step: 2520... Loss: 1.2791... Val Loss: 1.3508\n",
            "Epoch: 8/50... Step: 2530... Loss: 1.2747... Val Loss: 1.3519\n",
            "Epoch: 8/50... Step: 2540... Loss: 1.2978... Val Loss: 1.3598\n",
            "Epoch: 8/50... Step: 2550... Loss: 1.2872... Val Loss: 1.3530\n",
            "Epoch: 8/50... Step: 2560... Loss: 1.2503... Val Loss: 1.3489\n",
            "Epoch: 8/50... Step: 2570... Loss: 1.2940... Val Loss: 1.3518\n",
            "Epoch: 8/50... Step: 2580... Loss: 1.2901... Val Loss: 1.3488\n",
            "Epoch: 8/50... Step: 2590... Loss: 1.2787... Val Loss: 1.3451\n",
            "Epoch: 8/50... Step: 2600... Loss: 1.3087... Val Loss: 1.3498\n",
            "Epoch: 8/50... Step: 2610... Loss: 1.2770... Val Loss: 1.3456\n",
            "Epoch: 8/50... Step: 2620... Loss: 1.2937... Val Loss: 1.3488\n",
            "Epoch: 8/50... Step: 2630... Loss: 1.3255... Val Loss: 1.3451\n",
            "Epoch: 8/50... Step: 2640... Loss: 1.2673... Val Loss: 1.3433\n",
            "Epoch: 8/50... Step: 2650... Loss: 1.2713... Val Loss: 1.3442\n",
            "Epoch: 8/50... Step: 2660... Loss: 1.2847... Val Loss: 1.3443\n",
            "Epoch: 8/50... Step: 2670... Loss: 1.2371... Val Loss: 1.3427\n",
            "Epoch: 8/50... Step: 2680... Loss: 1.3067... Val Loss: 1.3426\n",
            "Epoch: 8/50... Step: 2690... Loss: 1.3156... Val Loss: 1.3413\n",
            "Epoch: 8/50... Step: 2700... Loss: 1.3185... Val Loss: 1.3438\n",
            "Epoch: 8/50... Step: 2710... Loss: 1.3205... Val Loss: 1.3437\n",
            "Epoch: 8/50... Step: 2720... Loss: 1.3051... Val Loss: 1.3357\n",
            "Epoch: 8/50... Step: 2730... Loss: 1.2556... Val Loss: 1.3356\n",
            "Epoch: 8/50... Step: 2740... Loss: 1.3114... Val Loss: 1.3375\n",
            "Epoch: 8/50... Step: 2750... Loss: 1.3128... Val Loss: 1.3360\n",
            "Epoch: 8/50... Step: 2760... Loss: 1.3121... Val Loss: 1.3351\n",
            "Epoch: 8/50... Step: 2770... Loss: 1.3325... Val Loss: 1.3323\n",
            "Epoch: 8/50... Step: 2780... Loss: 1.2550... Val Loss: 1.3346\n",
            "Epoch: 9/50... Step: 2790... Loss: 1.2911... Val Loss: 1.3377\n",
            "Epoch: 9/50... Step: 2800... Loss: 1.2843... Val Loss: 1.3397\n",
            "Epoch: 9/50... Step: 2810... Loss: 1.3185... Val Loss: 1.3321\n",
            "Epoch: 9/50... Step: 2820... Loss: 1.2982... Val Loss: 1.3321\n",
            "Epoch: 9/50... Step: 2830... Loss: 1.2814... Val Loss: 1.3302\n",
            "Epoch: 9/50... Step: 2840... Loss: 1.2852... Val Loss: 1.3252\n",
            "Epoch: 9/50... Step: 2850... Loss: 1.2784... Val Loss: 1.3347\n",
            "Epoch: 9/50... Step: 2860... Loss: 1.2919... Val Loss: 1.3301\n",
            "Epoch: 9/50... Step: 2870... Loss: 1.2386... Val Loss: 1.3310\n",
            "Epoch: 9/50... Step: 2880... Loss: 1.2795... Val Loss: 1.3265\n",
            "Epoch: 9/50... Step: 2890... Loss: 1.2318... Val Loss: 1.3256\n",
            "Epoch: 9/50... Step: 2900... Loss: 1.2291... Val Loss: 1.3261\n",
            "Epoch: 9/50... Step: 2910... Loss: 1.2380... Val Loss: 1.3263\n",
            "Epoch: 9/50... Step: 2920... Loss: 1.2960... Val Loss: 1.3264\n",
            "Epoch: 9/50... Step: 2930... Loss: 1.2127... Val Loss: 1.3245\n",
            "Epoch: 9/50... Step: 2940... Loss: 1.2616... Val Loss: 1.3227\n",
            "Epoch: 9/50... Step: 2950... Loss: 1.2718... Val Loss: 1.3232\n",
            "Epoch: 9/50... Step: 2960... Loss: 1.2816... Val Loss: 1.3284\n",
            "Epoch: 9/50... Step: 2970... Loss: 1.2718... Val Loss: 1.3279\n",
            "Epoch: 9/50... Step: 2980... Loss: 1.3025... Val Loss: 1.3222\n",
            "Epoch: 9/50... Step: 2990... Loss: 1.2578... Val Loss: 1.3256\n",
            "Epoch: 9/50... Step: 3000... Loss: 1.2747... Val Loss: 1.3226\n",
            "Epoch: 9/50... Step: 3010... Loss: 1.2835... Val Loss: 1.3219\n",
            "Epoch: 9/50... Step: 3020... Loss: 1.3031... Val Loss: 1.3266\n",
            "Epoch: 9/50... Step: 3030... Loss: 1.2679... Val Loss: 1.3252\n",
            "Epoch: 9/50... Step: 3040... Loss: 1.2544... Val Loss: 1.3183\n",
            "Epoch: 9/50... Step: 3050... Loss: 1.2735... Val Loss: 1.3175\n",
            "Epoch: 9/50... Step: 3060... Loss: 1.2748... Val Loss: 1.3238\n",
            "Epoch: 9/50... Step: 3070... Loss: 1.2616... Val Loss: 1.3167\n",
            "Epoch: 9/50... Step: 3080... Loss: 1.2275... Val Loss: 1.3150\n",
            "Epoch: 9/50... Step: 3090... Loss: 1.2626... Val Loss: 1.3154\n",
            "Epoch: 9/50... Step: 3100... Loss: 1.2370... Val Loss: 1.3186\n",
            "Epoch: 9/50... Step: 3110... Loss: 1.3027... Val Loss: 1.3170\n",
            "Epoch: 9/50... Step: 3120... Loss: 1.3052... Val Loss: 1.3144\n",
            "Epoch: 9/50... Step: 3130... Loss: 1.2498... Val Loss: 1.3277\n",
            "Epoch: 10/50... Step: 3140... Loss: 1.2452... Val Loss: 1.3162\n",
            "Epoch: 10/50... Step: 3150... Loss: 1.2425... Val Loss: 1.3128\n",
            "Epoch: 10/50... Step: 3160... Loss: 1.2451... Val Loss: 1.3103\n",
            "Epoch: 10/50... Step: 3170... Loss: 1.2765... Val Loss: 1.3092\n",
            "Epoch: 10/50... Step: 3180... Loss: 1.2874... Val Loss: 1.3105\n",
            "Epoch: 10/50... Step: 3190... Loss: 1.2514... Val Loss: 1.3101\n",
            "Epoch: 10/50... Step: 3200... Loss: 1.1913... Val Loss: 1.3080\n",
            "Epoch: 10/50... Step: 3210... Loss: 1.2363... Val Loss: 1.3108\n",
            "Epoch: 10/50... Step: 3220... Loss: 1.2288... Val Loss: 1.3251\n",
            "Epoch: 10/50... Step: 3230... Loss: 1.2619... Val Loss: 1.3100\n",
            "Epoch: 10/50... Step: 3240... Loss: 1.2448... Val Loss: 1.3161\n",
            "Epoch: 10/50... Step: 3250... Loss: 1.2802... Val Loss: 1.3117\n",
            "Epoch: 10/50... Step: 3260... Loss: 1.2125... Val Loss: 1.3092\n",
            "Epoch: 10/50... Step: 3270... Loss: 1.2656... Val Loss: 1.3063\n",
            "Epoch: 10/50... Step: 3280... Loss: 1.2203... Val Loss: 1.3116\n",
            "Epoch: 10/50... Step: 3290... Loss: 1.2460... Val Loss: 1.3068\n",
            "Epoch: 10/50... Step: 3300... Loss: 1.2395... Val Loss: 1.3041\n",
            "Epoch: 10/50... Step: 3310... Loss: 1.2392... Val Loss: 1.3018\n",
            "Epoch: 10/50... Step: 3320... Loss: 1.2288... Val Loss: 1.3129\n",
            "Epoch: 10/50... Step: 3330... Loss: 1.1917... Val Loss: 1.3047\n",
            "Epoch: 10/50... Step: 3340... Loss: 1.2291... Val Loss: 1.3058\n",
            "Epoch: 10/50... Step: 3350... Loss: 1.2642... Val Loss: 1.3052\n",
            "Epoch: 10/50... Step: 3360... Loss: 1.2831... Val Loss: 1.3040\n",
            "Epoch: 10/50... Step: 3370... Loss: 1.2567... Val Loss: 1.3029\n",
            "Epoch: 10/50... Step: 3380... Loss: 1.2478... Val Loss: 1.3043\n",
            "Epoch: 10/50... Step: 3390... Loss: 1.2752... Val Loss: 1.3040\n",
            "Epoch: 10/50... Step: 3400... Loss: 1.2796... Val Loss: 1.3064\n",
            "Epoch: 10/50... Step: 3410... Loss: 1.1955... Val Loss: 1.3057\n",
            "Epoch: 10/50... Step: 3420... Loss: 1.3027... Val Loss: 1.3001\n",
            "Epoch: 10/50... Step: 3430... Loss: 1.2469... Val Loss: 1.3028\n",
            "Epoch: 10/50... Step: 3440... Loss: 1.2070... Val Loss: 1.3011\n",
            "Epoch: 10/50... Step: 3450... Loss: 1.2554... Val Loss: 1.3090\n",
            "Epoch: 10/50... Step: 3460... Loss: 1.2713... Val Loss: 1.3012\n",
            "Epoch: 10/50... Step: 3470... Loss: 1.2438... Val Loss: 1.2977\n",
            "Epoch: 10/50... Step: 3480... Loss: 1.2984... Val Loss: 1.3034\n",
            "Epoch: 11/50... Step: 3490... Loss: 1.2450... Val Loss: 1.3042\n",
            "Epoch: 11/50... Step: 3500... Loss: 1.2433... Val Loss: 1.3014\n",
            "Epoch: 11/50... Step: 3510... Loss: 1.2397... Val Loss: 1.2944\n",
            "Epoch: 11/50... Step: 3520... Loss: 1.2315... Val Loss: 1.2948\n",
            "Epoch: 11/50... Step: 3530... Loss: 1.2429... Val Loss: 1.2961\n",
            "Epoch: 11/50... Step: 3540... Loss: 1.2418... Val Loss: 1.2965\n",
            "Epoch: 11/50... Step: 3550... Loss: 1.2088... Val Loss: 1.3030\n",
            "Epoch: 11/50... Step: 3560... Loss: 1.2291... Val Loss: 1.2967\n",
            "Epoch: 11/50... Step: 3570... Loss: 1.2085... Val Loss: 1.2998\n",
            "Epoch: 11/50... Step: 3580... Loss: 1.2277... Val Loss: 1.3000\n",
            "Epoch: 11/50... Step: 3590... Loss: 1.2284... Val Loss: 1.2940\n",
            "Epoch: 11/50... Step: 3600... Loss: 1.2087... Val Loss: 1.2986\n",
            "Epoch: 11/50... Step: 3610... Loss: 1.2419... Val Loss: 1.2950\n",
            "Epoch: 11/50... Step: 3620... Loss: 1.1906... Val Loss: 1.2960\n",
            "Epoch: 11/50... Step: 3630... Loss: 1.1776... Val Loss: 1.2946\n",
            "Epoch: 11/50... Step: 3640... Loss: 1.2329... Val Loss: 1.2948\n",
            "Epoch: 11/50... Step: 3650... Loss: 1.1803... Val Loss: 1.2934\n",
            "Epoch: 11/50... Step: 3660... Loss: 1.2165... Val Loss: 1.2853\n",
            "Epoch: 11/50... Step: 3670... Loss: 1.2776... Val Loss: 1.2954\n",
            "Epoch: 11/50... Step: 3680... Loss: 1.2741... Val Loss: 1.2931\n",
            "Epoch: 11/50... Step: 3690... Loss: 1.2128... Val Loss: 1.2945\n",
            "Epoch: 11/50... Step: 3700... Loss: 1.3099... Val Loss: 1.2919\n",
            "Epoch: 11/50... Step: 3710... Loss: 1.2232... Val Loss: 1.2870\n",
            "Epoch: 11/50... Step: 3720... Loss: 1.2079... Val Loss: 1.2937\n",
            "Epoch: 11/50... Step: 3730... Loss: 1.1968... Val Loss: 1.2965\n",
            "Epoch: 11/50... Step: 3740... Loss: 1.2054... Val Loss: 1.2921\n",
            "Epoch: 11/50... Step: 3750... Loss: 1.2308... Val Loss: 1.2980\n",
            "Epoch: 11/50... Step: 3760... Loss: 1.2204... Val Loss: 1.2914\n",
            "Epoch: 11/50... Step: 3770... Loss: 1.1926... Val Loss: 1.2902\n",
            "Epoch: 11/50... Step: 3780... Loss: 1.2325... Val Loss: 1.2912\n",
            "Epoch: 11/50... Step: 3790... Loss: 1.2319... Val Loss: 1.2943\n",
            "Epoch: 11/50... Step: 3800... Loss: 1.2367... Val Loss: 1.2873\n",
            "Epoch: 11/50... Step: 3810... Loss: 1.2035... Val Loss: 1.2948\n",
            "Epoch: 11/50... Step: 3820... Loss: 1.2455... Val Loss: 1.2879\n",
            "Epoch: 12/50... Step: 3830... Loss: 1.2410... Val Loss: 1.2995\n",
            "Epoch: 12/50... Step: 3840... Loss: 1.2177... Val Loss: 1.2943\n",
            "Epoch: 12/50... Step: 3850... Loss: 1.2518... Val Loss: 1.2898\n",
            "Epoch: 12/50... Step: 3860... Loss: 1.2120... Val Loss: 1.2882\n",
            "Epoch: 12/50... Step: 3870... Loss: 1.2023... Val Loss: 1.2900\n",
            "Epoch: 12/50... Step: 3880... Loss: 1.1805... Val Loss: 1.2865\n",
            "Epoch: 12/50... Step: 3890... Loss: 1.1754... Val Loss: 1.2866\n",
            "Epoch: 12/50... Step: 3900... Loss: 1.1703... Val Loss: 1.2849\n",
            "Epoch: 12/50... Step: 3910... Loss: 1.2112... Val Loss: 1.2875\n",
            "Epoch: 12/50... Step: 3920... Loss: 1.2354... Val Loss: 1.2862\n",
            "Epoch: 12/50... Step: 3930... Loss: 1.1726... Val Loss: 1.2840\n",
            "Epoch: 12/50... Step: 3940... Loss: 1.2291... Val Loss: 1.2868\n",
            "Epoch: 12/50... Step: 3950... Loss: 1.2189... Val Loss: 1.2895\n",
            "Epoch: 12/50... Step: 3960... Loss: 1.1232... Val Loss: 1.2896\n",
            "Epoch: 12/50... Step: 3970... Loss: 1.1516... Val Loss: 1.2891\n",
            "Epoch: 12/50... Step: 3980... Loss: 1.1705... Val Loss: 1.2819\n",
            "Epoch: 12/50... Step: 3990... Loss: 1.1814... Val Loss: 1.2860\n",
            "Epoch: 12/50... Step: 4000... Loss: 1.1791... Val Loss: 1.2842\n",
            "Epoch: 12/50... Step: 4010... Loss: 1.2239... Val Loss: 1.2832\n",
            "Epoch: 12/50... Step: 4020... Loss: 1.2074... Val Loss: 1.2850\n",
            "Epoch: 12/50... Step: 4030... Loss: 1.1897... Val Loss: 1.2845\n",
            "Epoch: 12/50... Step: 4040... Loss: 1.1866... Val Loss: 1.2841\n",
            "Epoch: 12/50... Step: 4050... Loss: 1.2063... Val Loss: 1.2844\n",
            "Epoch: 12/50... Step: 4060... Loss: 1.1903... Val Loss: 1.2800\n",
            "Epoch: 12/50... Step: 4070... Loss: 1.1806... Val Loss: 1.2802\n",
            "Epoch: 12/50... Step: 4080... Loss: 1.1881... Val Loss: 1.2857\n",
            "Epoch: 12/50... Step: 4090... Loss: 1.2149... Val Loss: 1.2815\n",
            "Epoch: 12/50... Step: 4100... Loss: 1.2252... Val Loss: 1.2806\n",
            "Epoch: 12/50... Step: 4110... Loss: 1.2026... Val Loss: 1.2817\n",
            "Epoch: 12/50... Step: 4120... Loss: 1.2114... Val Loss: 1.2799\n",
            "Epoch: 12/50... Step: 4130... Loss: 1.1904... Val Loss: 1.2790\n",
            "Epoch: 12/50... Step: 4140... Loss: 1.1954... Val Loss: 1.2809\n",
            "Epoch: 12/50... Step: 4150... Loss: 1.2292... Val Loss: 1.2844\n",
            "Epoch: 12/50... Step: 4160... Loss: 1.2462... Val Loss: 1.2811\n",
            "Epoch: 12/50... Step: 4170... Loss: 1.1947... Val Loss: 1.2837\n",
            "Epoch: 13/50... Step: 4180... Loss: 1.1784... Val Loss: 1.2791\n",
            "Epoch: 13/50... Step: 4190... Loss: 1.2483... Val Loss: 1.2803\n",
            "Epoch: 13/50... Step: 4200... Loss: 1.1801... Val Loss: 1.2820\n",
            "Epoch: 13/50... Step: 4210... Loss: 1.1678... Val Loss: 1.2844\n",
            "Epoch: 13/50... Step: 4220... Loss: 1.1884... Val Loss: 1.2813\n",
            "Epoch: 13/50... Step: 4230... Loss: 1.1814... Val Loss: 1.2816\n",
            "Epoch: 13/50... Step: 4240... Loss: 1.2024... Val Loss: 1.2839\n",
            "Epoch: 13/50... Step: 4250... Loss: 1.1993... Val Loss: 1.2758\n",
            "Epoch: 13/50... Step: 4260... Loss: 1.1744... Val Loss: 1.2757\n",
            "Epoch: 13/50... Step: 4270... Loss: 1.1465... Val Loss: 1.2802\n",
            "Epoch: 13/50... Step: 4280... Loss: 1.1899... Val Loss: 1.2796\n",
            "Epoch: 13/50... Step: 4290... Loss: 1.1691... Val Loss: 1.2798\n",
            "Epoch: 13/50... Step: 4300... Loss: 1.1395... Val Loss: 1.2768\n",
            "Epoch: 13/50... Step: 4310... Loss: 1.1867... Val Loss: 1.2774\n",
            "Epoch: 13/50... Step: 4320... Loss: 1.1817... Val Loss: 1.2773\n",
            "Epoch: 13/50... Step: 4330... Loss: 1.1986... Val Loss: 1.2739\n",
            "Epoch: 13/50... Step: 4340... Loss: 1.2081... Val Loss: 1.2753\n",
            "Epoch: 13/50... Step: 4350... Loss: 1.1816... Val Loss: 1.2734\n",
            "Epoch: 13/50... Step: 4360... Loss: 1.1672... Val Loss: 1.2774\n",
            "Epoch: 13/50... Step: 4370... Loss: 1.2222... Val Loss: 1.2745\n",
            "Epoch: 13/50... Step: 4380... Loss: 1.1491... Val Loss: 1.2765\n",
            "Epoch: 13/50... Step: 4390... Loss: 1.1676... Val Loss: 1.2786\n",
            "Epoch: 13/50... Step: 4400... Loss: 1.1884... Val Loss: 1.2747\n",
            "Epoch: 13/50... Step: 4410... Loss: 1.1353... Val Loss: 1.2714\n",
            "Epoch: 13/50... Step: 4420... Loss: 1.2220... Val Loss: 1.2743\n",
            "Epoch: 13/50... Step: 4430... Loss: 1.1995... Val Loss: 1.2793\n",
            "Epoch: 13/50... Step: 4440... Loss: 1.2046... Val Loss: 1.2754\n",
            "Epoch: 13/50... Step: 4450... Loss: 1.2066... Val Loss: 1.2758\n",
            "Epoch: 13/50... Step: 4460... Loss: 1.1831... Val Loss: 1.2725\n",
            "Epoch: 13/50... Step: 4470... Loss: 1.1551... Val Loss: 1.2739\n",
            "Epoch: 13/50... Step: 4480... Loss: 1.2066... Val Loss: 1.2781\n",
            "Epoch: 13/50... Step: 4490... Loss: 1.2026... Val Loss: 1.2817\n",
            "Epoch: 13/50... Step: 4500... Loss: 1.1998... Val Loss: 1.2727\n",
            "Epoch: 13/50... Step: 4510... Loss: 1.2306... Val Loss: 1.2697\n",
            "Epoch: 13/50... Step: 4520... Loss: 1.1462... Val Loss: 1.2709\n",
            "Epoch: 14/50... Step: 4530... Loss: 1.1926... Val Loss: 1.2739\n",
            "Epoch: 14/50... Step: 4540... Loss: 1.1832... Val Loss: 1.2720\n",
            "Epoch: 14/50... Step: 4550... Loss: 1.2130... Val Loss: 1.2678\n",
            "Epoch: 14/50... Step: 4560... Loss: 1.2020... Val Loss: 1.2665\n",
            "Epoch: 14/50... Step: 4570... Loss: 1.1784... Val Loss: 1.2739\n",
            "Epoch: 14/50... Step: 4580... Loss: 1.1802... Val Loss: 1.2677\n",
            "Epoch: 14/50... Step: 4590... Loss: 1.1712... Val Loss: 1.2693\n",
            "Epoch: 14/50... Step: 4600... Loss: 1.1910... Val Loss: 1.2707\n",
            "Epoch: 14/50... Step: 4610... Loss: 1.1343... Val Loss: 1.2676\n",
            "Epoch: 14/50... Step: 4620... Loss: 1.1875... Val Loss: 1.2705\n",
            "Epoch: 14/50... Step: 4630... Loss: 1.1322... Val Loss: 1.2743\n",
            "Epoch: 14/50... Step: 4640... Loss: 1.1401... Val Loss: 1.2705\n",
            "Epoch: 14/50... Step: 4650... Loss: 1.1470... Val Loss: 1.2671\n",
            "Epoch: 14/50... Step: 4660... Loss: 1.2069... Val Loss: 1.2665\n",
            "Epoch: 14/50... Step: 4670... Loss: 1.1266... Val Loss: 1.2701\n",
            "Epoch: 14/50... Step: 4680... Loss: 1.1631... Val Loss: 1.2646\n",
            "Epoch: 14/50... Step: 4690... Loss: 1.1799... Val Loss: 1.2672\n",
            "Epoch: 14/50... Step: 4700... Loss: 1.1926... Val Loss: 1.2684\n",
            "Epoch: 14/50... Step: 4710... Loss: 1.1844... Val Loss: 1.2694\n",
            "Epoch: 14/50... Step: 4720... Loss: 1.1951... Val Loss: 1.2661\n",
            "Epoch: 14/50... Step: 4730... Loss: 1.1414... Val Loss: 1.2628\n",
            "Epoch: 14/50... Step: 4740... Loss: 1.1786... Val Loss: 1.2708\n",
            "Epoch: 14/50... Step: 4750... Loss: 1.1989... Val Loss: 1.2733\n",
            "Epoch: 14/50... Step: 4760... Loss: 1.2113... Val Loss: 1.2675\n",
            "Epoch: 14/50... Step: 4770... Loss: 1.1800... Val Loss: 1.2656\n",
            "Epoch: 14/50... Step: 4780... Loss: 1.1533... Val Loss: 1.2677\n",
            "Epoch: 14/50... Step: 4790... Loss: 1.1814... Val Loss: 1.2669\n",
            "Epoch: 14/50... Step: 4800... Loss: 1.1825... Val Loss: 1.2690\n",
            "Epoch: 14/50... Step: 4810... Loss: 1.1765... Val Loss: 1.2656\n",
            "Epoch: 14/50... Step: 4820... Loss: 1.1272... Val Loss: 1.2681\n",
            "Epoch: 14/50... Step: 4830... Loss: 1.1771... Val Loss: 1.2660\n",
            "Epoch: 14/50... Step: 4840... Loss: 1.1604... Val Loss: 1.2651\n",
            "Epoch: 14/50... Step: 4850... Loss: 1.1979... Val Loss: 1.2622\n",
            "Epoch: 14/50... Step: 4860... Loss: 1.2017... Val Loss: 1.2659\n",
            "Epoch: 14/50... Step: 4870... Loss: 1.1589... Val Loss: 1.2674\n",
            "Epoch: 15/50... Step: 4880... Loss: 1.1683... Val Loss: 1.2675\n",
            "Epoch: 15/50... Step: 4890... Loss: 1.1698... Val Loss: 1.2620\n",
            "Epoch: 15/50... Step: 4900... Loss: 1.1618... Val Loss: 1.2643\n",
            "Epoch: 15/50... Step: 4910... Loss: 1.1834... Val Loss: 1.2614\n",
            "Epoch: 15/50... Step: 4920... Loss: 1.2059... Val Loss: 1.2670\n",
            "Epoch: 15/50... Step: 4930... Loss: 1.1436... Val Loss: 1.2625\n",
            "Epoch: 15/50... Step: 4940... Loss: 1.1063... Val Loss: 1.2629\n",
            "Epoch: 15/50... Step: 4950... Loss: 1.1477... Val Loss: 1.2612\n",
            "Epoch: 15/50... Step: 4960... Loss: 1.1388... Val Loss: 1.2611\n",
            "Epoch: 15/50... Step: 4970... Loss: 1.1696... Val Loss: 1.2677\n",
            "Epoch: 15/50... Step: 4980... Loss: 1.1734... Val Loss: 1.2693\n",
            "Epoch: 15/50... Step: 4990... Loss: 1.1830... Val Loss: 1.2613\n",
            "Epoch: 15/50... Step: 5000... Loss: 1.1418... Val Loss: 1.2629\n",
            "Epoch: 15/50... Step: 5010... Loss: 1.1846... Val Loss: 1.2612\n",
            "Epoch: 15/50... Step: 5020... Loss: 1.1437... Val Loss: 1.2647\n",
            "Epoch: 15/50... Step: 5030... Loss: 1.1648... Val Loss: 1.2626\n",
            "Epoch: 15/50... Step: 5040... Loss: 1.1671... Val Loss: 1.2612\n",
            "Epoch: 15/50... Step: 5050... Loss: 1.1558... Val Loss: 1.2572\n",
            "Epoch: 15/50... Step: 5060... Loss: 1.1503... Val Loss: 1.2685\n",
            "Epoch: 15/50... Step: 5070... Loss: 1.1219... Val Loss: 1.2600\n",
            "Epoch: 15/50... Step: 5080... Loss: 1.1645... Val Loss: 1.2575\n",
            "Epoch: 15/50... Step: 5090... Loss: 1.1756... Val Loss: 1.2605\n",
            "Epoch: 15/50... Step: 5100... Loss: 1.2019... Val Loss: 1.2628\n",
            "Epoch: 15/50... Step: 5110... Loss: 1.1709... Val Loss: 1.2626\n",
            "Epoch: 15/50... Step: 5120... Loss: 1.1587... Val Loss: 1.2575\n",
            "Epoch: 15/50... Step: 5130... Loss: 1.2031... Val Loss: 1.2582\n",
            "Epoch: 15/50... Step: 5140... Loss: 1.1990... Val Loss: 1.2621\n",
            "Epoch: 15/50... Step: 5150... Loss: 1.1086... Val Loss: 1.2698\n",
            "Epoch: 15/50... Step: 5160... Loss: 1.2312... Val Loss: 1.2617\n",
            "Epoch: 15/50... Step: 5170... Loss: 1.1566... Val Loss: 1.2665\n",
            "Epoch: 15/50... Step: 5180... Loss: 1.1305... Val Loss: 1.2648\n",
            "Epoch: 15/50... Step: 5190... Loss: 1.1760... Val Loss: 1.2687\n",
            "Epoch: 15/50... Step: 5200... Loss: 1.1797... Val Loss: 1.2583\n",
            "Epoch: 15/50... Step: 5210... Loss: 1.1697... Val Loss: 1.2621\n",
            "Epoch: 15/50... Step: 5220... Loss: 1.2226... Val Loss: 1.2613\n",
            "Epoch: 16/50... Step: 5230... Loss: 1.1523... Val Loss: 1.2655\n",
            "Epoch: 16/50... Step: 5240... Loss: 1.1735... Val Loss: 1.2658\n",
            "Epoch: 16/50... Step: 5250... Loss: 1.1614... Val Loss: 1.2619\n",
            "Epoch: 16/50... Step: 5260... Loss: 1.1597... Val Loss: 1.2577\n",
            "Epoch: 16/50... Step: 5270... Loss: 1.1638... Val Loss: 1.2646\n",
            "Epoch: 16/50... Step: 5280... Loss: 1.1709... Val Loss: 1.2607\n",
            "Epoch: 16/50... Step: 5290... Loss: 1.1279... Val Loss: 1.2570\n",
            "Epoch: 16/50... Step: 5300... Loss: 1.1568... Val Loss: 1.2599\n",
            "Epoch: 16/50... Step: 5310... Loss: 1.1504... Val Loss: 1.2617\n",
            "Epoch: 16/50... Step: 5320... Loss: 1.1346... Val Loss: 1.2660\n",
            "Epoch: 16/50... Step: 5330... Loss: 1.1489... Val Loss: 1.2618\n",
            "Epoch: 16/50... Step: 5340... Loss: 1.1578... Val Loss: 1.2604\n",
            "Epoch: 16/50... Step: 5350... Loss: 1.1675... Val Loss: 1.2605\n",
            "Epoch: 16/50... Step: 5360... Loss: 1.1063... Val Loss: 1.2596\n",
            "Epoch: 16/50... Step: 5370... Loss: 1.1265... Val Loss: 1.2619\n",
            "Epoch: 16/50... Step: 5380... Loss: 1.1592... Val Loss: 1.2576\n",
            "Epoch: 16/50... Step: 5390... Loss: 1.1276... Val Loss: 1.2557\n",
            "Epoch: 16/50... Step: 5400... Loss: 1.1491... Val Loss: 1.2553\n",
            "Epoch: 16/50... Step: 5410... Loss: 1.1931... Val Loss: 1.2614\n",
            "Epoch: 16/50... Step: 5420... Loss: 1.1919... Val Loss: 1.2590\n",
            "Epoch: 16/50... Step: 5430... Loss: 1.1407... Val Loss: 1.2584\n",
            "Epoch: 16/50... Step: 5440... Loss: 1.2276... Val Loss: 1.2551\n",
            "Epoch: 16/50... Step: 5450... Loss: 1.1528... Val Loss: 1.2598\n",
            "Epoch: 16/50... Step: 5460... Loss: 1.1396... Val Loss: 1.2609\n",
            "Epoch: 16/50... Step: 5470... Loss: 1.1292... Val Loss: 1.2567\n",
            "Epoch: 16/50... Step: 5480... Loss: 1.1460... Val Loss: 1.2547\n",
            "Epoch: 16/50... Step: 5490... Loss: 1.1426... Val Loss: 1.2557\n",
            "Epoch: 16/50... Step: 5500... Loss: 1.1494... Val Loss: 1.2584\n",
            "Epoch: 16/50... Step: 5510... Loss: 1.1150... Val Loss: 1.2568\n",
            "Epoch: 16/50... Step: 5520... Loss: 1.1522... Val Loss: 1.2556\n",
            "Epoch: 16/50... Step: 5530... Loss: 1.1701... Val Loss: 1.2593\n",
            "Epoch: 16/50... Step: 5540... Loss: 1.1622... Val Loss: 1.2597\n",
            "Epoch: 16/50... Step: 5550... Loss: 1.1315... Val Loss: 1.2576\n",
            "Epoch: 16/50... Step: 5560... Loss: 1.1885... Val Loss: 1.2597\n",
            "Epoch: 17/50... Step: 5570... Loss: 1.1649... Val Loss: 1.2611\n",
            "Epoch: 17/50... Step: 5580... Loss: 1.1435... Val Loss: 1.2623\n",
            "Epoch: 17/50... Step: 5590... Loss: 1.1899... Val Loss: 1.2579\n",
            "Epoch: 17/50... Step: 5600... Loss: 1.1405... Val Loss: 1.2547\n",
            "Epoch: 17/50... Step: 5610... Loss: 1.1357... Val Loss: 1.2541\n",
            "Epoch: 17/50... Step: 5620... Loss: 1.1298... Val Loss: 1.2612\n",
            "Epoch: 17/50... Step: 5630... Loss: 1.1216... Val Loss: 1.2582\n",
            "Epoch: 17/50... Step: 5640... Loss: 1.1046... Val Loss: 1.2590\n",
            "Epoch: 17/50... Step: 5650... Loss: 1.1453... Val Loss: 1.2565\n",
            "Epoch: 17/50... Step: 5660... Loss: 1.1719... Val Loss: 1.2625\n",
            "Epoch: 17/50... Step: 5670... Loss: 1.1187... Val Loss: 1.2574\n",
            "Epoch: 17/50... Step: 5680... Loss: 1.1533... Val Loss: 1.2606\n",
            "Epoch: 17/50... Step: 5690... Loss: 1.1372... Val Loss: 1.2627\n",
            "Epoch: 17/50... Step: 5700... Loss: 1.0621... Val Loss: 1.2624\n",
            "Epoch: 17/50... Step: 5710... Loss: 1.1002... Val Loss: 1.2552\n",
            "Epoch: 17/50... Step: 5720... Loss: 1.1105... Val Loss: 1.2524\n",
            "Epoch: 17/50... Step: 5730... Loss: 1.1277... Val Loss: 1.2587\n",
            "Epoch: 17/50... Step: 5740... Loss: 1.1143... Val Loss: 1.2532\n",
            "Epoch: 17/50... Step: 5750... Loss: 1.1728... Val Loss: 1.2514\n",
            "Epoch: 17/50... Step: 5760... Loss: 1.1488... Val Loss: 1.2542\n",
            "Epoch: 17/50... Step: 5770... Loss: 1.1301... Val Loss: 1.2594\n",
            "Epoch: 17/50... Step: 5780... Loss: 1.1382... Val Loss: 1.2514\n",
            "Epoch: 17/50... Step: 5790... Loss: 1.1393... Val Loss: 1.2567\n",
            "Epoch: 17/50... Step: 5800... Loss: 1.1345... Val Loss: 1.2560\n",
            "Epoch: 17/50... Step: 5810... Loss: 1.1339... Val Loss: 1.2600\n",
            "Epoch: 17/50... Step: 5820... Loss: 1.1300... Val Loss: 1.2621\n",
            "Epoch: 17/50... Step: 5830... Loss: 1.1677... Val Loss: 1.2523\n",
            "Epoch: 17/50... Step: 5840... Loss: 1.1657... Val Loss: 1.2539\n",
            "Epoch: 17/50... Step: 5850... Loss: 1.1481... Val Loss: 1.2607\n",
            "Epoch: 17/50... Step: 5860... Loss: 1.1403... Val Loss: 1.2548\n",
            "Epoch: 17/50... Step: 5870... Loss: 1.1150... Val Loss: 1.2552\n",
            "Epoch: 17/50... Step: 5880... Loss: 1.1256... Val Loss: 1.2562\n",
            "Epoch: 17/50... Step: 5890... Loss: 1.1673... Val Loss: 1.2613\n",
            "Epoch: 17/50... Step: 5900... Loss: 1.1791... Val Loss: 1.2509\n",
            "Epoch: 17/50... Step: 5910... Loss: 1.1303... Val Loss: 1.2473\n",
            "Epoch: 18/50... Step: 5920... Loss: 1.1149... Val Loss: 1.2594\n",
            "Epoch: 18/50... Step: 5930... Loss: 1.1855... Val Loss: 1.2580\n",
            "Epoch: 18/50... Step: 5940... Loss: 1.1072... Val Loss: 1.2579\n",
            "Epoch: 18/50... Step: 5950... Loss: 1.1042... Val Loss: 1.2541\n",
            "Epoch: 18/50... Step: 5960... Loss: 1.1449... Val Loss: 1.2579\n",
            "Epoch: 18/50... Step: 5970... Loss: 1.1291... Val Loss: 1.2513\n",
            "Epoch: 18/50... Step: 5980... Loss: 1.1250... Val Loss: 1.2524\n",
            "Epoch: 18/50... Step: 5990... Loss: 1.1365... Val Loss: 1.2484\n",
            "Epoch: 18/50... Step: 6000... Loss: 1.1110... Val Loss: 1.2482\n",
            "Epoch: 18/50... Step: 6010... Loss: 1.0932... Val Loss: 1.2551\n",
            "Epoch: 18/50... Step: 6020... Loss: 1.1174... Val Loss: 1.2523\n",
            "Epoch: 18/50... Step: 6030... Loss: 1.1227... Val Loss: 1.2525\n",
            "Epoch: 18/50... Step: 6040... Loss: 1.0818... Val Loss: 1.2577\n",
            "Epoch: 18/50... Step: 6050... Loss: 1.1374... Val Loss: 1.2540\n",
            "Epoch: 18/50... Step: 6060... Loss: 1.1222... Val Loss: 1.2566\n",
            "Epoch: 18/50... Step: 6070... Loss: 1.1175... Val Loss: 1.2515\n",
            "Epoch: 18/50... Step: 6080... Loss: 1.1601... Val Loss: 1.2573\n",
            "Epoch: 18/50... Step: 6090... Loss: 1.1252... Val Loss: 1.2480\n",
            "Epoch: 18/50... Step: 6100... Loss: 1.1174... Val Loss: 1.2496\n",
            "Epoch: 18/50... Step: 6110... Loss: 1.1623... Val Loss: 1.2476\n",
            "Epoch: 18/50... Step: 6120... Loss: 1.1074... Val Loss: 1.2515\n",
            "Epoch: 18/50... Step: 6130... Loss: 1.1247... Val Loss: 1.2544\n",
            "Epoch: 18/50... Step: 6140... Loss: 1.1324... Val Loss: 1.2495\n",
            "Epoch: 18/50... Step: 6150... Loss: 1.0845... Val Loss: 1.2503\n",
            "Epoch: 18/50... Step: 6160... Loss: 1.1637... Val Loss: 1.2553\n",
            "Epoch: 18/50... Step: 6170... Loss: 1.1418... Val Loss: 1.2551\n",
            "Epoch: 18/50... Step: 6180... Loss: 1.1514... Val Loss: 1.2524\n",
            "Epoch: 18/50... Step: 6190... Loss: 1.1446... Val Loss: 1.2501\n",
            "Epoch: 18/50... Step: 6200... Loss: 1.1335... Val Loss: 1.2545\n",
            "Epoch: 18/50... Step: 6210... Loss: 1.0986... Val Loss: 1.2526\n",
            "Epoch: 18/50... Step: 6220... Loss: 1.1549... Val Loss: 1.2490\n",
            "Epoch: 18/50... Step: 6230... Loss: 1.1517... Val Loss: 1.2521\n",
            "Epoch: 18/50... Step: 6240... Loss: 1.1545... Val Loss: 1.2535\n",
            "Epoch: 18/50... Step: 6250... Loss: 1.1709... Val Loss: 1.2574\n",
            "Epoch: 18/50... Step: 6260... Loss: 1.0873... Val Loss: 1.2513\n",
            "Epoch: 19/50... Step: 6270... Loss: 1.1379... Val Loss: 1.2559\n",
            "Epoch: 19/50... Step: 6280... Loss: 1.1143... Val Loss: 1.2535\n",
            "Epoch: 19/50... Step: 6290... Loss: 1.1624... Val Loss: 1.2516\n",
            "Epoch: 19/50... Step: 6300... Loss: 1.1379... Val Loss: 1.2520\n",
            "Epoch: 19/50... Step: 6310... Loss: 1.1183... Val Loss: 1.2549\n",
            "Epoch: 19/50... Step: 6320... Loss: 1.1317... Val Loss: 1.2515\n",
            "Epoch: 19/50... Step: 6330... Loss: 1.1147... Val Loss: 1.2540\n",
            "Epoch: 19/50... Step: 6340... Loss: 1.1374... Val Loss: 1.2509\n",
            "Epoch: 19/50... Step: 6350... Loss: 1.0808... Val Loss: 1.2479\n",
            "Epoch: 19/50... Step: 6360... Loss: 1.1226... Val Loss: 1.2531\n",
            "Epoch: 19/50... Step: 6370... Loss: 1.0908... Val Loss: 1.2503\n",
            "Epoch: 19/50... Step: 6380... Loss: 1.0929... Val Loss: 1.2517\n",
            "Epoch: 19/50... Step: 6390... Loss: 1.0964... Val Loss: 1.2566\n",
            "Epoch: 19/50... Step: 6400... Loss: 1.1538... Val Loss: 1.2508\n",
            "Epoch: 19/50... Step: 6410... Loss: 1.0971... Val Loss: 1.2503\n",
            "Epoch: 19/50... Step: 6420... Loss: 1.1184... Val Loss: 1.2453\n",
            "Epoch: 19/50... Step: 6430... Loss: 1.1260... Val Loss: 1.2530\n",
            "Epoch: 19/50... Step: 6440... Loss: 1.1332... Val Loss: 1.2472\n",
            "Epoch: 19/50... Step: 6450... Loss: 1.1341... Val Loss: 1.2468\n",
            "Epoch: 19/50... Step: 6460... Loss: 1.1473... Val Loss: 1.2437\n",
            "Epoch: 19/50... Step: 6470... Loss: 1.0964... Val Loss: 1.2442\n",
            "Epoch: 19/50... Step: 6480... Loss: 1.1414... Val Loss: 1.2520\n",
            "Epoch: 19/50... Step: 6490... Loss: 1.1509... Val Loss: 1.2449\n",
            "Epoch: 19/50... Step: 6500... Loss: 1.1453... Val Loss: 1.2471\n",
            "Epoch: 19/50... Step: 6510... Loss: 1.1431... Val Loss: 1.2453\n",
            "Epoch: 19/50... Step: 6520... Loss: 1.1165... Val Loss: 1.2501\n",
            "Epoch: 19/50... Step: 6530... Loss: 1.1337... Val Loss: 1.2498\n",
            "Epoch: 19/50... Step: 6540... Loss: 1.1373... Val Loss: 1.2475\n",
            "Epoch: 19/50... Step: 6550... Loss: 1.1387... Val Loss: 1.2462\n",
            "Epoch: 19/50... Step: 6560... Loss: 1.0902... Val Loss: 1.2531\n",
            "Epoch: 19/50... Step: 6570... Loss: 1.1213... Val Loss: 1.2461\n",
            "Epoch: 19/50... Step: 6580... Loss: 1.1110... Val Loss: 1.2526\n",
            "Epoch: 19/50... Step: 6590... Loss: 1.1495... Val Loss: 1.2512\n",
            "Epoch: 19/50... Step: 6600... Loss: 1.1552... Val Loss: 1.2514\n",
            "Epoch: 19/50... Step: 6610... Loss: 1.1069... Val Loss: 1.2558\n",
            "Epoch: 20/50... Step: 6620... Loss: 1.1294... Val Loss: 1.2568\n",
            "Epoch: 20/50... Step: 6630... Loss: 1.1199... Val Loss: 1.2489\n",
            "Epoch: 20/50... Step: 6640... Loss: 1.1160... Val Loss: 1.2527\n",
            "Epoch: 20/50... Step: 6650... Loss: 1.1347... Val Loss: 1.2560\n",
            "Epoch: 20/50... Step: 6660... Loss: 1.1489... Val Loss: 1.2550\n",
            "Epoch: 20/50... Step: 6670... Loss: 1.1012... Val Loss: 1.2506\n",
            "Epoch: 20/50... Step: 6680... Loss: 1.0780... Val Loss: 1.2509\n",
            "Epoch: 20/50... Step: 6690... Loss: 1.1020... Val Loss: 1.2496\n",
            "Epoch: 20/50... Step: 6700... Loss: 1.0982... Val Loss: 1.2481\n",
            "Epoch: 20/50... Step: 6710... Loss: 1.1189... Val Loss: 1.2515\n",
            "Epoch: 20/50... Step: 6720... Loss: 1.1126... Val Loss: 1.2483\n",
            "Epoch: 20/50... Step: 6730... Loss: 1.1378... Val Loss: 1.2491\n",
            "Epoch: 20/50... Step: 6740... Loss: 1.0909... Val Loss: 1.2432\n",
            "Epoch: 20/50... Step: 6750... Loss: 1.1238... Val Loss: 1.2467\n",
            "Epoch: 20/50... Step: 6760... Loss: 1.1104... Val Loss: 1.2487\n",
            "Epoch: 20/50... Step: 6770... Loss: 1.1049... Val Loss: 1.2467\n",
            "Epoch: 20/50... Step: 6780... Loss: 1.1235... Val Loss: 1.2492\n",
            "Epoch: 20/50... Step: 6790... Loss: 1.1130... Val Loss: 1.2455\n",
            "Epoch: 20/50... Step: 6800... Loss: 1.1050... Val Loss: 1.2488\n",
            "Epoch: 20/50... Step: 6810... Loss: 1.0803... Val Loss: 1.2478\n",
            "Epoch: 20/50... Step: 6820... Loss: 1.1130... Val Loss: 1.2465\n",
            "Epoch: 20/50... Step: 6830... Loss: 1.1331... Val Loss: 1.2501\n",
            "Epoch: 20/50... Step: 6840... Loss: 1.1555... Val Loss: 1.2431\n",
            "Epoch: 20/50... Step: 6850... Loss: 1.1086... Val Loss: 1.2467\n",
            "Epoch: 20/50... Step: 6860... Loss: 1.1165... Val Loss: 1.2456\n",
            "Epoch: 20/50... Step: 6870... Loss: 1.1274... Val Loss: 1.2483\n",
            "Epoch: 20/50... Step: 6880... Loss: 1.1441... Val Loss: 1.2492\n",
            "Epoch: 20/50... Step: 6890... Loss: 1.0687... Val Loss: 1.2477\n",
            "Epoch: 20/50... Step: 6900... Loss: 1.1730... Val Loss: 1.2436\n",
            "Epoch: 20/50... Step: 6910... Loss: 1.1076... Val Loss: 1.2486\n",
            "Epoch: 20/50... Step: 6920... Loss: 1.0763... Val Loss: 1.2481\n",
            "Epoch: 20/50... Step: 6930... Loss: 1.1193... Val Loss: 1.2498\n",
            "Epoch: 20/50... Step: 6940... Loss: 1.1414... Val Loss: 1.2446\n",
            "Epoch: 20/50... Step: 6950... Loss: 1.1390... Val Loss: 1.2452\n",
            "Epoch: 20/50... Step: 6960... Loss: 1.1800... Val Loss: 1.2500\n",
            "Epoch: 21/50... Step: 6970... Loss: 1.1190... Val Loss: 1.2530\n",
            "Epoch: 21/50... Step: 6980... Loss: 1.1397... Val Loss: 1.2486\n",
            "Epoch: 21/50... Step: 6990... Loss: 1.1241... Val Loss: 1.2519\n",
            "Epoch: 21/50... Step: 7000... Loss: 1.1227... Val Loss: 1.2510\n",
            "Epoch: 21/50... Step: 7010... Loss: 1.1145... Val Loss: 1.2529\n",
            "Epoch: 21/50... Step: 7020... Loss: 1.1373... Val Loss: 1.2518\n",
            "Epoch: 21/50... Step: 7030... Loss: 1.0907... Val Loss: 1.2508\n",
            "Epoch: 21/50... Step: 7040... Loss: 1.1180... Val Loss: 1.2501\n",
            "Epoch: 21/50... Step: 7050... Loss: 1.0894... Val Loss: 1.2479\n",
            "Epoch: 21/50... Step: 7060... Loss: 1.1053... Val Loss: 1.2521\n",
            "Epoch: 21/50... Step: 7070... Loss: 1.1044... Val Loss: 1.2496\n",
            "Epoch: 21/50... Step: 7080... Loss: 1.0969... Val Loss: 1.2495\n",
            "Epoch: 21/50... Step: 7090... Loss: 1.1301... Val Loss: 1.2477\n",
            "Epoch: 21/50... Step: 7100... Loss: 1.0679... Val Loss: 1.2524\n",
            "Epoch: 21/50... Step: 7110... Loss: 1.0814... Val Loss: 1.2484\n",
            "Epoch: 21/50... Step: 7120... Loss: 1.1201... Val Loss: 1.2440\n",
            "Epoch: 21/50... Step: 7130... Loss: 1.0817... Val Loss: 1.2443\n",
            "Epoch: 21/50... Step: 7140... Loss: 1.1002... Val Loss: 1.2431\n",
            "Epoch: 21/50... Step: 7150... Loss: 1.1678... Val Loss: 1.2464\n",
            "Epoch: 21/50... Step: 7160... Loss: 1.1577... Val Loss: 1.2443\n",
            "Epoch: 21/50... Step: 7170... Loss: 1.0913... Val Loss: 1.2464\n",
            "Epoch: 21/50... Step: 7180... Loss: 1.1771... Val Loss: 1.2454\n",
            "Epoch: 21/50... Step: 7190... Loss: 1.0993... Val Loss: 1.2465\n",
            "Epoch: 21/50... Step: 7200... Loss: 1.1148... Val Loss: 1.2466\n",
            "Epoch: 21/50... Step: 7210... Loss: 1.0775... Val Loss: 1.2452\n",
            "Epoch: 21/50... Step: 7220... Loss: 1.1040... Val Loss: 1.2472\n",
            "Epoch: 21/50... Step: 7230... Loss: 1.1158... Val Loss: 1.2465\n",
            "Epoch: 21/50... Step: 7240... Loss: 1.1197... Val Loss: 1.2488\n",
            "Epoch: 21/50... Step: 7250... Loss: 1.0704... Val Loss: 1.2490\n",
            "Epoch: 21/50... Step: 7260... Loss: 1.1118... Val Loss: 1.2464\n",
            "Epoch: 21/50... Step: 7270... Loss: 1.1246... Val Loss: 1.2518\n",
            "Epoch: 21/50... Step: 7280... Loss: 1.1146... Val Loss: 1.2513\n",
            "Epoch: 21/50... Step: 7290... Loss: 1.0907... Val Loss: 1.2510\n",
            "Epoch: 21/50... Step: 7300... Loss: 1.1326... Val Loss: 1.2402\n",
            "Epoch: 22/50... Step: 7310... Loss: 1.1472... Val Loss: 1.2603\n",
            "Epoch: 22/50... Step: 7320... Loss: 1.1155... Val Loss: 1.2515\n",
            "Epoch: 22/50... Step: 7330... Loss: 1.1612... Val Loss: 1.2487\n",
            "Epoch: 22/50... Step: 7340... Loss: 1.1092... Val Loss: 1.2495\n",
            "Epoch: 22/50... Step: 7350... Loss: 1.1036... Val Loss: 1.2457\n",
            "Epoch: 22/50... Step: 7360... Loss: 1.0830... Val Loss: 1.2505\n",
            "Epoch: 22/50... Step: 7370... Loss: 1.0644... Val Loss: 1.2470\n",
            "Epoch: 22/50... Step: 7380... Loss: 1.0569... Val Loss: 1.2490\n",
            "Epoch: 22/50... Step: 7390... Loss: 1.1103... Val Loss: 1.2512\n",
            "Epoch: 22/50... Step: 7400... Loss: 1.1252... Val Loss: 1.2495\n",
            "Epoch: 22/50... Step: 7410... Loss: 1.0807... Val Loss: 1.2447\n",
            "Epoch: 22/50... Step: 7420... Loss: 1.1127... Val Loss: 1.2457\n",
            "Epoch: 22/50... Step: 7430... Loss: 1.0985... Val Loss: 1.2494\n",
            "Epoch: 22/50... Step: 7440... Loss: 1.0204... Val Loss: 1.2543\n",
            "Epoch: 22/50... Step: 7450... Loss: 1.0584... Val Loss: 1.2456\n",
            "Epoch: 22/50... Step: 7460... Loss: 1.0812... Val Loss: 1.2522\n",
            "Epoch: 22/50... Step: 7470... Loss: 1.0741... Val Loss: 1.2461\n",
            "Epoch: 22/50... Step: 7480... Loss: 1.0690... Val Loss: 1.2454\n",
            "Epoch: 22/50... Step: 7490... Loss: 1.1328... Val Loss: 1.2456\n",
            "Epoch: 22/50... Step: 7500... Loss: 1.1150... Val Loss: 1.2493\n",
            "Epoch: 22/50... Step: 7510... Loss: 1.0892... Val Loss: 1.2512\n",
            "Epoch: 22/50... Step: 7520... Loss: 1.0847... Val Loss: 1.2520\n",
            "Epoch: 22/50... Step: 7530... Loss: 1.1045... Val Loss: 1.2477\n",
            "Epoch: 22/50... Step: 7540... Loss: 1.0974... Val Loss: 1.2438\n",
            "Epoch: 22/50... Step: 7550... Loss: 1.0953... Val Loss: 1.2429\n",
            "Epoch: 22/50... Step: 7560... Loss: 1.0873... Val Loss: 1.2448\n",
            "Epoch: 22/50... Step: 7570... Loss: 1.1128... Val Loss: 1.2454\n",
            "Epoch: 22/50... Step: 7580... Loss: 1.1320... Val Loss: 1.2435\n",
            "Epoch: 22/50... Step: 7590... Loss: 1.1088... Val Loss: 1.2477\n",
            "Epoch: 22/50... Step: 7600... Loss: 1.1015... Val Loss: 1.2393\n",
            "Epoch: 22/50... Step: 7610... Loss: 1.0781... Val Loss: 1.2497\n",
            "Epoch: 22/50... Step: 7620... Loss: 1.0918... Val Loss: 1.2460\n",
            "Epoch: 22/50... Step: 7630... Loss: 1.1243... Val Loss: 1.2424\n",
            "Epoch: 22/50... Step: 7640... Loss: 1.1428... Val Loss: 1.2458\n",
            "Epoch: 22/50... Step: 7650... Loss: 1.0960... Val Loss: 1.2491\n",
            "Epoch: 23/50... Step: 7660... Loss: 1.0664... Val Loss: 1.2455\n",
            "Epoch: 23/50... Step: 7670... Loss: 1.1381... Val Loss: 1.2521\n",
            "Epoch: 23/50... Step: 7680... Loss: 1.0821... Val Loss: 1.2478\n",
            "Epoch: 23/50... Step: 7690... Loss: 1.0731... Val Loss: 1.2513\n",
            "Epoch: 23/50... Step: 7700... Loss: 1.0986... Val Loss: 1.2447\n",
            "Epoch: 23/50... Step: 7710... Loss: 1.0846... Val Loss: 1.2466\n",
            "Epoch: 23/50... Step: 7720... Loss: 1.1047... Val Loss: 1.2463\n",
            "Epoch: 23/50... Step: 7730... Loss: 1.1079... Val Loss: 1.2465\n",
            "Epoch: 23/50... Step: 7740... Loss: 1.0788... Val Loss: 1.2471\n",
            "Epoch: 23/50... Step: 7750... Loss: 1.0449... Val Loss: 1.2486\n",
            "Epoch: 23/50... Step: 7760... Loss: 1.0934... Val Loss: 1.2488\n",
            "Epoch: 23/50... Step: 7770... Loss: 1.0897... Val Loss: 1.2462\n",
            "Epoch: 23/50... Step: 7780... Loss: 1.0456... Val Loss: 1.2447\n",
            "Epoch: 23/50... Step: 7790... Loss: 1.1001... Val Loss: 1.2471\n",
            "Epoch: 23/50... Step: 7800... Loss: 1.1026... Val Loss: 1.2438\n",
            "Epoch: 23/50... Step: 7810... Loss: 1.0946... Val Loss: 1.2460\n",
            "Epoch: 23/50... Step: 7820... Loss: 1.1283... Val Loss: 1.2499\n",
            "Epoch: 23/50... Step: 7830... Loss: 1.0856... Val Loss: 1.2423\n",
            "Epoch: 23/50... Step: 7840... Loss: 1.0798... Val Loss: 1.2427\n",
            "Epoch: 23/50... Step: 7850... Loss: 1.1197... Val Loss: 1.2405\n",
            "Epoch: 23/50... Step: 7860... Loss: 1.0697... Val Loss: 1.2444\n",
            "Epoch: 23/50... Step: 7870... Loss: 1.0812... Val Loss: 1.2454\n",
            "Epoch: 23/50... Step: 7880... Loss: 1.1020... Val Loss: 1.2462\n",
            "Epoch: 23/50... Step: 7890... Loss: 1.0531... Val Loss: 1.2462\n",
            "Epoch: 23/50... Step: 7900... Loss: 1.1239... Val Loss: 1.2444\n",
            "Epoch: 23/50... Step: 7910... Loss: 1.1109... Val Loss: 1.2525\n",
            "Epoch: 23/50... Step: 7920... Loss: 1.1034... Val Loss: 1.2488\n",
            "Epoch: 23/50... Step: 7930... Loss: 1.1002... Val Loss: 1.2443\n",
            "Epoch: 23/50... Step: 7940... Loss: 1.1028... Val Loss: 1.2531\n",
            "Epoch: 23/50... Step: 7950... Loss: 1.0693... Val Loss: 1.2415\n",
            "Epoch: 23/50... Step: 7960... Loss: 1.1141... Val Loss: 1.2488\n",
            "Epoch: 23/50... Step: 7970... Loss: 1.1289... Val Loss: 1.2517\n",
            "Epoch: 23/50... Step: 7980... Loss: 1.1316... Val Loss: 1.2431\n",
            "Epoch: 23/50... Step: 7990... Loss: 1.1276... Val Loss: 1.2488\n",
            "Epoch: 23/50... Step: 8000... Loss: 1.0606... Val Loss: 1.2474\n",
            "Epoch: 24/50... Step: 8010... Loss: 1.1055... Val Loss: 1.2494\n",
            "Epoch: 24/50... Step: 8020... Loss: 1.0906... Val Loss: 1.2531\n",
            "Epoch: 24/50... Step: 8030... Loss: 1.1243... Val Loss: 1.2513\n",
            "Epoch: 24/50... Step: 8040... Loss: 1.1138... Val Loss: 1.2459\n",
            "Epoch: 24/50... Step: 8050... Loss: 1.0937... Val Loss: 1.2396\n",
            "Epoch: 24/50... Step: 8060... Loss: 1.0817... Val Loss: 1.2476\n",
            "Epoch: 24/50... Step: 8070... Loss: 1.0849... Val Loss: 1.2450\n",
            "Epoch: 24/50... Step: 8080... Loss: 1.1161... Val Loss: 1.2464\n",
            "Epoch: 24/50... Step: 8090... Loss: 1.0495... Val Loss: 1.2475\n",
            "Epoch: 24/50... Step: 8100... Loss: 1.1003... Val Loss: 1.2471\n",
            "Epoch: 24/50... Step: 8110... Loss: 1.0618... Val Loss: 1.2480\n",
            "Epoch: 24/50... Step: 8120... Loss: 1.0620... Val Loss: 1.2472\n",
            "Epoch: 24/50... Step: 8130... Loss: 1.0513... Val Loss: 1.2461\n",
            "Epoch: 24/50... Step: 8140... Loss: 1.1039... Val Loss: 1.2477\n",
            "Epoch: 24/50... Step: 8150... Loss: 1.0681... Val Loss: 1.2480\n",
            "Epoch: 24/50... Step: 8160... Loss: 1.0850... Val Loss: 1.2422\n",
            "Epoch: 24/50... Step: 8170... Loss: 1.0963... Val Loss: 1.2515\n",
            "Epoch: 24/50... Step: 8180... Loss: 1.1045... Val Loss: 1.2473\n",
            "Epoch: 24/50... Step: 8190... Loss: 1.0992... Val Loss: 1.2435\n",
            "Epoch: 24/50... Step: 8200... Loss: 1.1078... Val Loss: 1.2421\n",
            "Epoch: 24/50... Step: 8210... Loss: 1.0543... Val Loss: 1.2428\n",
            "Epoch: 24/50... Step: 8220... Loss: 1.1054... Val Loss: 1.2496\n",
            "Epoch: 24/50... Step: 8230... Loss: 1.1248... Val Loss: 1.2425\n",
            "Epoch: 24/50... Step: 8240... Loss: 1.1213... Val Loss: 1.2456\n",
            "Epoch: 24/50... Step: 8250... Loss: 1.0985... Val Loss: 1.2431\n",
            "Epoch: 24/50... Step: 8260... Loss: 1.0704... Val Loss: 1.2428\n",
            "Epoch: 24/50... Step: 8270... Loss: 1.0845... Val Loss: 1.2497\n",
            "Epoch: 24/50... Step: 8280... Loss: 1.0964... Val Loss: 1.2446\n",
            "Epoch: 24/50... Step: 8290... Loss: 1.0933... Val Loss: 1.2488\n",
            "Epoch: 24/50... Step: 8300... Loss: 1.0650... Val Loss: 1.2449\n",
            "Epoch: 24/50... Step: 8310... Loss: 1.0907... Val Loss: 1.2419\n",
            "Epoch: 24/50... Step: 8320... Loss: 1.0871... Val Loss: 1.2496\n",
            "Epoch: 24/50... Step: 8330... Loss: 1.1057... Val Loss: 1.2427\n",
            "Epoch: 24/50... Step: 8340... Loss: 1.1227... Val Loss: 1.2424\n",
            "Epoch: 24/50... Step: 8350... Loss: 1.0921... Val Loss: 1.2445\n",
            "Epoch: 25/50... Step: 8360... Loss: 1.0812... Val Loss: 1.2503\n",
            "Epoch: 25/50... Step: 8370... Loss: 1.0813... Val Loss: 1.2437\n",
            "Epoch: 25/50... Step: 8380... Loss: 1.0840... Val Loss: 1.2447\n",
            "Epoch: 25/50... Step: 8390... Loss: 1.1045... Val Loss: 1.2473\n",
            "Epoch: 25/50... Step: 8400... Loss: 1.1245... Val Loss: 1.2435\n",
            "Epoch: 25/50... Step: 8410... Loss: 1.0690... Val Loss: 1.2475\n",
            "Epoch: 25/50... Step: 8420... Loss: 1.0411... Val Loss: 1.2446\n",
            "Epoch: 25/50... Step: 8430... Loss: 1.0675... Val Loss: 1.2487\n",
            "Epoch: 25/50... Step: 8440... Loss: 1.0605... Val Loss: 1.2440\n",
            "Epoch: 25/50... Step: 8450... Loss: 1.0919... Val Loss: 1.2467\n",
            "Epoch: 25/50... Step: 8460... Loss: 1.0811... Val Loss: 1.2459\n",
            "Epoch: 25/50... Step: 8470... Loss: 1.1082... Val Loss: 1.2434\n",
            "Epoch: 25/50... Step: 8480... Loss: 1.0709... Val Loss: 1.2449\n",
            "Epoch: 25/50... Step: 8490... Loss: 1.0838... Val Loss: 1.2432\n",
            "Epoch: 25/50... Step: 8500... Loss: 1.0655... Val Loss: 1.2443\n",
            "Epoch: 25/50... Step: 8510... Loss: 1.0782... Val Loss: 1.2412\n",
            "Epoch: 25/50... Step: 8520... Loss: 1.0867... Val Loss: 1.2464\n",
            "Epoch: 25/50... Step: 8530... Loss: 1.0923... Val Loss: 1.2418\n",
            "Epoch: 25/50... Step: 8540... Loss: 1.0709... Val Loss: 1.2442\n",
            "Epoch: 25/50... Step: 8550... Loss: 1.0403... Val Loss: 1.2357\n",
            "Epoch: 25/50... Step: 8560... Loss: 1.0881... Val Loss: 1.2391\n",
            "Epoch: 25/50... Step: 8570... Loss: 1.1038... Val Loss: 1.2490\n",
            "Epoch: 25/50... Step: 8580... Loss: 1.1260... Val Loss: 1.2390\n",
            "Epoch: 25/50... Step: 8590... Loss: 1.0717... Val Loss: 1.2441\n",
            "Epoch: 25/50... Step: 8600... Loss: 1.1029... Val Loss: 1.2402\n",
            "Epoch: 25/50... Step: 8610... Loss: 1.1036... Val Loss: 1.2411\n",
            "Epoch: 25/50... Step: 8620... Loss: 1.1091... Val Loss: 1.2425\n",
            "Epoch: 25/50... Step: 8630... Loss: 1.0386... Val Loss: 1.2451\n",
            "Epoch: 25/50... Step: 8640... Loss: 1.1391... Val Loss: 1.2427\n",
            "Epoch: 25/50... Step: 8650... Loss: 1.0741... Val Loss: 1.2495\n",
            "Epoch: 25/50... Step: 8660... Loss: 1.0492... Val Loss: 1.2404\n",
            "Epoch: 25/50... Step: 8670... Loss: 1.0952... Val Loss: 1.2492\n",
            "Epoch: 25/50... Step: 8680... Loss: 1.1119... Val Loss: 1.2460\n",
            "Epoch: 25/50... Step: 8690... Loss: 1.0836... Val Loss: 1.2429\n",
            "Epoch: 25/50... Step: 8700... Loss: 1.1417... Val Loss: 1.2434\n",
            "Epoch: 26/50... Step: 8710... Loss: 1.0856... Val Loss: 1.2498\n",
            "Epoch: 26/50... Step: 8720... Loss: 1.0991... Val Loss: 1.2476\n",
            "Epoch: 26/50... Step: 8730... Loss: 1.0971... Val Loss: 1.2456\n",
            "Epoch: 26/50... Step: 8740... Loss: 1.0885... Val Loss: 1.2445\n",
            "Epoch: 26/50... Step: 8750... Loss: 1.0964... Val Loss: 1.2480\n",
            "Epoch: 26/50... Step: 8760... Loss: 1.0992... Val Loss: 1.2437\n",
            "Epoch: 26/50... Step: 8770... Loss: 1.0416... Val Loss: 1.2445\n",
            "Epoch: 26/50... Step: 8780... Loss: 1.1017... Val Loss: 1.2462\n",
            "Epoch: 26/50... Step: 8790... Loss: 1.0781... Val Loss: 1.2453\n",
            "Epoch: 26/50... Step: 8800... Loss: 1.0644... Val Loss: 1.2484\n",
            "Epoch: 26/50... Step: 8810... Loss: 1.0732... Val Loss: 1.2481\n",
            "Epoch: 26/50... Step: 8820... Loss: 1.0790... Val Loss: 1.2492\n",
            "Epoch: 26/50... Step: 8830... Loss: 1.0905... Val Loss: 1.2466\n",
            "Epoch: 26/50... Step: 8840... Loss: 1.0294... Val Loss: 1.2444\n",
            "Epoch: 26/50... Step: 8850... Loss: 1.0426... Val Loss: 1.2493\n",
            "Epoch: 26/50... Step: 8860... Loss: 1.0999... Val Loss: 1.2463\n",
            "Epoch: 26/50... Step: 8870... Loss: 1.0556... Val Loss: 1.2499\n",
            "Epoch: 26/50... Step: 8880... Loss: 1.0642... Val Loss: 1.2450\n",
            "Epoch: 26/50... Step: 8890... Loss: 1.1181... Val Loss: 1.2427\n",
            "Epoch: 26/50... Step: 8900... Loss: 1.1104... Val Loss: 1.2432\n",
            "Epoch: 26/50... Step: 8910... Loss: 1.0813... Val Loss: 1.2421\n",
            "Epoch: 26/50... Step: 8920... Loss: 1.1470... Val Loss: 1.2470\n",
            "Epoch: 26/50... Step: 8930... Loss: 1.0792... Val Loss: 1.2405\n",
            "Epoch: 26/50... Step: 8940... Loss: 1.0888... Val Loss: 1.2502\n",
            "Epoch: 26/50... Step: 8950... Loss: 1.0513... Val Loss: 1.2451\n",
            "Epoch: 26/50... Step: 8960... Loss: 1.0647... Val Loss: 1.2433\n",
            "Epoch: 26/50... Step: 8970... Loss: 1.0900... Val Loss: 1.2441\n",
            "Epoch: 26/50... Step: 8980... Loss: 1.0857... Val Loss: 1.2468\n",
            "Epoch: 26/50... Step: 8990... Loss: 1.0526... Val Loss: 1.2400\n",
            "Epoch: 26/50... Step: 9000... Loss: 1.0697... Val Loss: 1.2461\n",
            "Epoch: 26/50... Step: 9010... Loss: 1.0960... Val Loss: 1.2414\n",
            "Epoch: 26/50... Step: 9020... Loss: 1.0873... Val Loss: 1.2472\n",
            "Epoch: 26/50... Step: 9030... Loss: 1.0680... Val Loss: 1.2461\n",
            "Epoch: 26/50... Step: 9040... Loss: 1.1052... Val Loss: 1.2409\n",
            "Epoch: 27/50... Step: 9050... Loss: 1.1011... Val Loss: 1.2420\n",
            "Epoch: 27/50... Step: 9060... Loss: 1.0682... Val Loss: 1.2524\n",
            "Epoch: 27/50... Step: 9070... Loss: 1.1083... Val Loss: 1.2477\n",
            "Epoch: 27/50... Step: 9080... Loss: 1.0658... Val Loss: 1.2469\n",
            "Epoch: 27/50... Step: 9090... Loss: 1.0659... Val Loss: 1.2431\n",
            "Epoch: 27/50... Step: 9100... Loss: 1.0575... Val Loss: 1.2507\n",
            "Epoch: 27/50... Step: 9110... Loss: 1.0410... Val Loss: 1.2431\n",
            "Epoch: 27/50... Step: 9120... Loss: 1.0432... Val Loss: 1.2500\n",
            "Epoch: 27/50... Step: 9130... Loss: 1.0807... Val Loss: 1.2422\n",
            "Epoch: 27/50... Step: 9140... Loss: 1.0923... Val Loss: 1.2432\n",
            "Epoch: 27/50... Step: 9150... Loss: 1.0473... Val Loss: 1.2483\n",
            "Epoch: 27/50... Step: 9160... Loss: 1.0968... Val Loss: 1.2407\n",
            "Epoch: 27/50... Step: 9170... Loss: 1.0798... Val Loss: 1.2491\n",
            "Epoch: 27/50... Step: 9180... Loss: 1.0192... Val Loss: 1.2441\n",
            "Epoch: 27/50... Step: 9190... Loss: 1.0400... Val Loss: 1.2476\n",
            "Epoch: 27/50... Step: 9200... Loss: 1.0617... Val Loss: 1.2475\n",
            "Epoch: 27/50... Step: 9210... Loss: 1.0564... Val Loss: 1.2394\n",
            "Epoch: 27/50... Step: 9220... Loss: 1.0388... Val Loss: 1.2413\n",
            "Epoch: 27/50... Step: 9230... Loss: 1.0930... Val Loss: 1.2473\n",
            "Epoch: 27/50... Step: 9240... Loss: 1.0632... Val Loss: 1.2446\n",
            "Epoch: 27/50... Step: 9250... Loss: 1.0699... Val Loss: 1.2404\n",
            "Epoch: 27/50... Step: 9260... Loss: 1.0696... Val Loss: 1.2412\n",
            "Epoch: 27/50... Step: 9270... Loss: 1.0822... Val Loss: 1.2445\n",
            "Epoch: 27/50... Step: 9280... Loss: 1.0690... Val Loss: 1.2397\n",
            "Epoch: 27/50... Step: 9290... Loss: 1.0738... Val Loss: 1.2424\n",
            "Epoch: 27/50... Step: 9300... Loss: 1.0591... Val Loss: 1.2492\n",
            "Epoch: 27/50... Step: 9310... Loss: 1.0896... Val Loss: 1.2437\n",
            "Epoch: 27/50... Step: 9320... Loss: 1.0986... Val Loss: 1.2431\n",
            "Epoch: 27/50... Step: 9330... Loss: 1.0711... Val Loss: 1.2467\n",
            "Epoch: 27/50... Step: 9340... Loss: 1.0814... Val Loss: 1.2395\n",
            "Epoch: 27/50... Step: 9350... Loss: 1.0585... Val Loss: 1.2464\n",
            "Epoch: 27/50... Step: 9360... Loss: 1.0598... Val Loss: 1.2414\n",
            "Epoch: 27/50... Step: 9370... Loss: 1.1090... Val Loss: 1.2425\n",
            "Epoch: 27/50... Step: 9380... Loss: 1.1320... Val Loss: 1.2440\n",
            "Epoch: 27/50... Step: 9390... Loss: 1.0841... Val Loss: 1.2446\n",
            "Epoch: 28/50... Step: 9400... Loss: 1.0632... Val Loss: 1.2451\n",
            "Epoch: 28/50... Step: 9410... Loss: 1.1238... Val Loss: 1.2519\n",
            "Epoch: 28/50... Step: 9420... Loss: 1.0467... Val Loss: 1.2434\n",
            "Epoch: 28/50... Step: 9430... Loss: 1.0474... Val Loss: 1.2495\n",
            "Epoch: 28/50... Step: 9440... Loss: 1.0733... Val Loss: 1.2421\n",
            "Epoch: 28/50... Step: 9450... Loss: 1.0648... Val Loss: 1.2482\n",
            "Epoch: 28/50... Step: 9460... Loss: 1.0767... Val Loss: 1.2401\n",
            "Epoch: 28/50... Step: 9470... Loss: 1.0922... Val Loss: 1.2504\n",
            "Epoch: 28/50... Step: 9480... Loss: 1.0510... Val Loss: 1.2471\n",
            "Epoch: 28/50... Step: 9490... Loss: 1.0257... Val Loss: 1.2480\n",
            "Epoch: 28/50... Step: 9500... Loss: 1.0525... Val Loss: 1.2493\n",
            "Epoch: 28/50... Step: 9510... Loss: 1.0569... Val Loss: 1.2476\n",
            "Epoch: 28/50... Step: 9520... Loss: 1.0133... Val Loss: 1.2478\n",
            "Epoch: 28/50... Step: 9530... Loss: 1.0700... Val Loss: 1.2462\n",
            "Epoch: 28/50... Step: 9540... Loss: 1.0793... Val Loss: 1.2527\n",
            "Epoch: 28/50... Step: 9550... Loss: 1.0667... Val Loss: 1.2467\n",
            "Epoch: 28/50... Step: 9560... Loss: 1.0857... Val Loss: 1.2515\n",
            "Epoch: 28/50... Step: 9570... Loss: 1.0429... Val Loss: 1.2433\n",
            "Epoch: 28/50... Step: 9580... Loss: 1.0484... Val Loss: 1.2522\n",
            "Epoch: 28/50... Step: 9590... Loss: 1.0948... Val Loss: 1.2459\n",
            "Epoch: 28/50... Step: 9600... Loss: 1.0336... Val Loss: 1.2485\n",
            "Epoch: 28/50... Step: 9610... Loss: 1.0670... Val Loss: 1.2447\n",
            "Epoch: 28/50... Step: 9620... Loss: 1.0751... Val Loss: 1.2490\n",
            "Epoch: 28/50... Step: 9630... Loss: 1.0324... Val Loss: 1.2419\n",
            "Epoch: 28/50... Step: 9640... Loss: 1.0882... Val Loss: 1.2449\n",
            "Epoch: 28/50... Step: 9650... Loss: 1.0805... Val Loss: 1.2466\n",
            "Epoch: 28/50... Step: 9660... Loss: 1.0697... Val Loss: 1.2412\n",
            "Epoch: 28/50... Step: 9670... Loss: 1.0834... Val Loss: 1.2469\n",
            "Epoch: 28/50... Step: 9680... Loss: 1.0672... Val Loss: 1.2508\n",
            "Epoch: 28/50... Step: 9690... Loss: 1.0326... Val Loss: 1.2394\n",
            "Epoch: 28/50... Step: 9700... Loss: 1.0838... Val Loss: 1.2485\n",
            "Epoch: 28/50... Step: 9710... Loss: 1.0934... Val Loss: 1.2410\n",
            "Epoch: 28/50... Step: 9720... Loss: 1.0790... Val Loss: 1.2452\n",
            "Epoch: 28/50... Step: 9730... Loss: 1.1140... Val Loss: 1.2416\n",
            "Epoch: 28/50... Step: 9740... Loss: 1.0374... Val Loss: 1.2427\n",
            "Epoch: 29/50... Step: 9750... Loss: 1.0750... Val Loss: 1.2443\n",
            "Epoch: 29/50... Step: 9760... Loss: 1.0649... Val Loss: 1.2523\n",
            "Epoch: 29/50... Step: 9770... Loss: 1.0967... Val Loss: 1.2501\n",
            "Epoch: 29/50... Step: 9780... Loss: 1.0790... Val Loss: 1.2511\n",
            "Epoch: 29/50... Step: 9790... Loss: 1.0625... Val Loss: 1.2471\n",
            "Epoch: 29/50... Step: 9800... Loss: 1.0579... Val Loss: 1.2496\n",
            "Epoch: 29/50... Step: 9810... Loss: 1.0505... Val Loss: 1.2424\n",
            "Epoch: 29/50... Step: 9820... Loss: 1.0880... Val Loss: 1.2505\n",
            "Epoch: 29/50... Step: 9830... Loss: 1.0271... Val Loss: 1.2473\n",
            "Epoch: 29/50... Step: 9840... Loss: 1.0629... Val Loss: 1.2471\n",
            "Epoch: 29/50... Step: 9850... Loss: 1.0310... Val Loss: 1.2506\n",
            "Epoch: 29/50... Step: 9860... Loss: 1.0478... Val Loss: 1.2517\n",
            "Epoch: 29/50... Step: 9870... Loss: 1.0250... Val Loss: 1.2452\n",
            "Epoch: 29/50... Step: 9880... Loss: 1.0854... Val Loss: 1.2459\n",
            "Epoch: 29/50... Step: 9890... Loss: 1.0293... Val Loss: 1.2457\n",
            "Epoch: 29/50... Step: 9900... Loss: 1.0649... Val Loss: 1.2485\n",
            "Epoch: 29/50... Step: 9910... Loss: 1.0784... Val Loss: 1.2470\n",
            "Epoch: 29/50... Step: 9920... Loss: 1.0782... Val Loss: 1.2405\n",
            "Epoch: 29/50... Step: 9930... Loss: 1.0662... Val Loss: 1.2475\n",
            "Epoch: 29/50... Step: 9940... Loss: 1.0820... Val Loss: 1.2476\n",
            "Epoch: 29/50... Step: 9950... Loss: 1.0404... Val Loss: 1.2471\n",
            "Epoch: 29/50... Step: 9960... Loss: 1.0859... Val Loss: 1.2421\n",
            "Epoch: 29/50... Step: 9970... Loss: 1.0952... Val Loss: 1.2461\n",
            "Epoch: 29/50... Step: 9980... Loss: 1.0949... Val Loss: 1.2458\n",
            "Epoch: 29/50... Step: 9990... Loss: 1.0865... Val Loss: 1.2393\n",
            "Epoch: 29/50... Step: 10000... Loss: 1.0519... Val Loss: 1.2494\n",
            "Epoch: 29/50... Step: 10010... Loss: 1.0615... Val Loss: 1.2483\n",
            "Epoch: 29/50... Step: 10020... Loss: 1.0699... Val Loss: 1.2427\n",
            "Epoch: 29/50... Step: 10030... Loss: 1.0744... Val Loss: 1.2491\n",
            "Epoch: 29/50... Step: 10040... Loss: 1.0324... Val Loss: 1.2436\n",
            "Epoch: 29/50... Step: 10050... Loss: 1.0623... Val Loss: 1.2427\n",
            "Epoch: 29/50... Step: 10060... Loss: 1.0570... Val Loss: 1.2445\n",
            "Epoch: 29/50... Step: 10070... Loss: 1.1034... Val Loss: 1.2440\n",
            "Epoch: 29/50... Step: 10080... Loss: 1.0892... Val Loss: 1.2381\n",
            "Epoch: 29/50... Step: 10090... Loss: 1.0641... Val Loss: 1.2501\n",
            "Epoch: 30/50... Step: 10100... Loss: 1.0530... Val Loss: 1.2462\n",
            "Epoch: 30/50... Step: 10110... Loss: 1.0489... Val Loss: 1.2458\n",
            "Epoch: 30/50... Step: 10120... Loss: 1.0648... Val Loss: 1.2472\n",
            "Epoch: 30/50... Step: 10130... Loss: 1.0599... Val Loss: 1.2511\n",
            "Epoch: 30/50... Step: 10140... Loss: 1.0953... Val Loss: 1.2484\n",
            "Epoch: 30/50... Step: 10150... Loss: 1.0459... Val Loss: 1.2521\n",
            "Epoch: 30/50... Step: 10160... Loss: 1.0046... Val Loss: 1.2493\n",
            "Epoch: 30/50... Step: 10170... Loss: 1.0546... Val Loss: 1.2501\n",
            "Epoch: 30/50... Step: 10180... Loss: 1.0392... Val Loss: 1.2514\n",
            "Epoch: 30/50... Step: 10190... Loss: 1.0614... Val Loss: 1.2492\n",
            "Epoch: 30/50... Step: 10200... Loss: 1.0587... Val Loss: 1.2507\n",
            "Epoch: 30/50... Step: 10210... Loss: 1.0909... Val Loss: 1.2475\n",
            "Epoch: 30/50... Step: 10220... Loss: 1.0404... Val Loss: 1.2487\n",
            "Epoch: 30/50... Step: 10230... Loss: 1.0609... Val Loss: 1.2533\n",
            "Epoch: 30/50... Step: 10240... Loss: 1.0415... Val Loss: 1.2503\n",
            "Epoch: 30/50... Step: 10250... Loss: 1.0525... Val Loss: 1.2476\n",
            "Epoch: 30/50... Step: 10260... Loss: 1.0580... Val Loss: 1.2509\n",
            "Epoch: 30/50... Step: 10270... Loss: 1.0629... Val Loss: 1.2396\n",
            "Epoch: 30/50... Step: 10280... Loss: 1.0428... Val Loss: 1.2465\n",
            "Epoch: 30/50... Step: 10290... Loss: 1.0285... Val Loss: 1.2466\n",
            "Epoch: 30/50... Step: 10300... Loss: 1.0504... Val Loss: 1.2466\n",
            "Epoch: 30/50... Step: 10310... Loss: 1.0678... Val Loss: 1.2467\n",
            "Epoch: 30/50... Step: 10320... Loss: 1.1106... Val Loss: 1.2466\n",
            "Epoch: 30/50... Step: 10330... Loss: 1.0636... Val Loss: 1.2453\n",
            "Epoch: 30/50... Step: 10340... Loss: 1.0639... Val Loss: 1.2422\n",
            "Epoch: 30/50... Step: 10350... Loss: 1.0815... Val Loss: 1.2444\n",
            "Epoch: 30/50... Step: 10360... Loss: 1.0810... Val Loss: 1.2521\n",
            "Epoch: 30/50... Step: 10370... Loss: 1.0128... Val Loss: 1.2471\n",
            "Epoch: 30/50... Step: 10380... Loss: 1.1224... Val Loss: 1.2516\n",
            "Epoch: 30/50... Step: 10390... Loss: 1.0454... Val Loss: 1.2452\n",
            "Epoch: 30/50... Step: 10400... Loss: 1.0267... Val Loss: 1.2433\n",
            "Epoch: 30/50... Step: 10410... Loss: 1.0664... Val Loss: 1.2527\n",
            "Epoch: 30/50... Step: 10420... Loss: 1.0810... Val Loss: 1.2441\n",
            "Epoch: 30/50... Step: 10430... Loss: 1.0715... Val Loss: 1.2451\n",
            "Epoch: 30/50... Step: 10440... Loss: 1.1343... Val Loss: 1.2525\n",
            "Epoch: 31/50... Step: 10450... Loss: 1.0595... Val Loss: 1.2494\n",
            "Epoch: 31/50... Step: 10460... Loss: 1.0741... Val Loss: 1.2518\n",
            "Epoch: 31/50... Step: 10470... Loss: 1.0712... Val Loss: 1.2480\n",
            "Epoch: 31/50... Step: 10480... Loss: 1.0695... Val Loss: 1.2469\n",
            "Epoch: 31/50... Step: 10490... Loss: 1.0591... Val Loss: 1.2509\n",
            "Epoch: 31/50... Step: 10500... Loss: 1.0767... Val Loss: 1.2518\n",
            "Epoch: 31/50... Step: 10510... Loss: 1.0314... Val Loss: 1.2511\n",
            "Epoch: 31/50... Step: 10520... Loss: 1.0556... Val Loss: 1.2479\n",
            "Epoch: 31/50... Step: 10530... Loss: 1.0474... Val Loss: 1.2507\n",
            "Epoch: 31/50... Step: 10540... Loss: 1.0416... Val Loss: 1.2503\n",
            "Epoch: 31/50... Step: 10550... Loss: 1.0586... Val Loss: 1.2486\n",
            "Epoch: 31/50... Step: 10560... Loss: 1.0472... Val Loss: 1.2515\n",
            "Epoch: 31/50... Step: 10570... Loss: 1.0706... Val Loss: 1.2479\n",
            "Epoch: 31/50... Step: 10580... Loss: 1.0178... Val Loss: 1.2497\n",
            "Epoch: 31/50... Step: 10590... Loss: 1.0199... Val Loss: 1.2461\n",
            "Epoch: 31/50... Step: 10600... Loss: 1.0740... Val Loss: 1.2497\n",
            "Epoch: 31/50... Step: 10610... Loss: 1.0420... Val Loss: 1.2516\n",
            "Epoch: 31/50... Step: 10620... Loss: 1.0515... Val Loss: 1.2418\n",
            "Epoch: 31/50... Step: 10630... Loss: 1.0930... Val Loss: 1.2468\n",
            "Epoch: 31/50... Step: 10640... Loss: 1.1072... Val Loss: 1.2494\n",
            "Epoch: 31/50... Step: 10650... Loss: 1.0546... Val Loss: 1.2455\n",
            "Epoch: 31/50... Step: 10660... Loss: 1.1272... Val Loss: 1.2514\n",
            "Epoch: 31/50... Step: 10670... Loss: 1.0532... Val Loss: 1.2477\n",
            "Epoch: 31/50... Step: 10680... Loss: 1.0503... Val Loss: 1.2480\n",
            "Epoch: 31/50... Step: 10690... Loss: 1.0278... Val Loss: 1.2455\n",
            "Epoch: 31/50... Step: 10700... Loss: 1.0593... Val Loss: 1.2442\n",
            "Epoch: 31/50... Step: 10710... Loss: 1.0569... Val Loss: 1.2466\n",
            "Epoch: 31/50... Step: 10720... Loss: 1.0548... Val Loss: 1.2445\n",
            "Epoch: 31/50... Step: 10730... Loss: 1.0091... Val Loss: 1.2488\n",
            "Epoch: 31/50... Step: 10740... Loss: 1.0649... Val Loss: 1.2490\n",
            "Epoch: 31/50... Step: 10750... Loss: 1.0591... Val Loss: 1.2423\n",
            "Epoch: 31/50... Step: 10760... Loss: 1.0547... Val Loss: 1.2518\n",
            "Epoch: 31/50... Step: 10770... Loss: 1.0391... Val Loss: 1.2435\n",
            "Epoch: 31/50... Step: 10780... Loss: 1.0663... Val Loss: 1.2432\n",
            "Epoch: 32/50... Step: 10790... Loss: 1.1035... Val Loss: 1.2502\n",
            "Epoch: 32/50... Step: 10800... Loss: 1.0499... Val Loss: 1.2528\n",
            "Epoch: 32/50... Step: 10810... Loss: 1.0873... Val Loss: 1.2473\n",
            "Epoch: 32/50... Step: 10820... Loss: 1.0557... Val Loss: 1.2553\n",
            "Epoch: 32/50... Step: 10830... Loss: 1.0511... Val Loss: 1.2523\n",
            "Epoch: 32/50... Step: 10840... Loss: 1.0413... Val Loss: 1.2492\n",
            "Epoch: 32/50... Step: 10850... Loss: 1.0286... Val Loss: 1.2518\n",
            "Epoch: 32/50... Step: 10860... Loss: 1.0215... Val Loss: 1.2506\n",
            "Epoch: 32/50... Step: 10870... Loss: 1.0434... Val Loss: 1.2493\n",
            "Epoch: 32/50... Step: 10880... Loss: 1.0801... Val Loss: 1.2561\n",
            "Epoch: 32/50... Step: 10890... Loss: 1.0315... Val Loss: 1.2529\n",
            "Epoch: 32/50... Step: 10900... Loss: 1.0841... Val Loss: 1.2482\n",
            "Epoch: 32/50... Step: 10910... Loss: 1.0371... Val Loss: 1.2478\n",
            "Epoch: 32/50... Step: 10920... Loss: 0.9835... Val Loss: 1.2488\n",
            "Epoch: 32/50... Step: 10930... Loss: 1.0166... Val Loss: 1.2492\n",
            "Epoch: 32/50... Step: 10940... Loss: 1.0231... Val Loss: 1.2582\n",
            "Epoch: 32/50... Step: 10950... Loss: 1.0468... Val Loss: 1.2460\n",
            "Epoch: 32/50... Step: 10960... Loss: 1.0218... Val Loss: 1.2525\n",
            "Epoch: 32/50... Step: 10970... Loss: 1.0763... Val Loss: 1.2408\n",
            "Epoch: 32/50... Step: 10980... Loss: 1.0651... Val Loss: 1.2498\n",
            "Epoch: 32/50... Step: 10990... Loss: 1.0518... Val Loss: 1.2445\n",
            "Epoch: 32/50... Step: 11000... Loss: 1.0352... Val Loss: 1.2471\n",
            "Epoch: 32/50... Step: 11010... Loss: 1.0548... Val Loss: 1.2522\n",
            "Epoch: 32/50... Step: 11020... Loss: 1.0427... Val Loss: 1.2457\n",
            "Epoch: 32/50... Step: 11030... Loss: 1.0443... Val Loss: 1.2554\n",
            "Epoch: 32/50... Step: 11040... Loss: 1.0361... Val Loss: 1.2466\n",
            "Epoch: 32/50... Step: 11050... Loss: 1.0642... Val Loss: 1.2426\n",
            "Epoch: 32/50... Step: 11060... Loss: 1.0666... Val Loss: 1.2500\n",
            "Epoch: 32/50... Step: 11070... Loss: 1.0606... Val Loss: 1.2494\n",
            "Epoch: 32/50... Step: 11080... Loss: 1.0539... Val Loss: 1.2464\n",
            "Epoch: 32/50... Step: 11090... Loss: 1.0483... Val Loss: 1.2476\n",
            "Epoch: 32/50... Step: 11100... Loss: 1.0556... Val Loss: 1.2424\n",
            "Epoch: 32/50... Step: 11110... Loss: 1.0711... Val Loss: 1.2483\n",
            "Epoch: 32/50... Step: 11120... Loss: 1.0910... Val Loss: 1.2434\n",
            "Epoch: 32/50... Step: 11130... Loss: 1.0478... Val Loss: 1.2481\n",
            "Epoch: 33/50... Step: 11140... Loss: 1.0278... Val Loss: 1.2415\n",
            "Epoch: 33/50... Step: 11150... Loss: 1.0782... Val Loss: 1.2532\n",
            "Epoch: 33/50... Step: 11160... Loss: 1.0409... Val Loss: 1.2461\n",
            "Epoch: 33/50... Step: 11170... Loss: 1.0095... Val Loss: 1.2527\n",
            "Epoch: 33/50... Step: 11180... Loss: 1.0363... Val Loss: 1.2501\n",
            "Epoch: 33/50... Step: 11190... Loss: 1.0279... Val Loss: 1.2487\n",
            "Epoch: 33/50... Step: 11200... Loss: 1.0423... Val Loss: 1.2455\n",
            "Epoch: 33/50... Step: 11210... Loss: 1.0593... Val Loss: 1.2492\n",
            "Epoch: 33/50... Step: 11220... Loss: 1.0307... Val Loss: 1.2461\n",
            "Epoch: 33/50... Step: 11230... Loss: 1.0121... Val Loss: 1.2483\n",
            "Epoch: 33/50... Step: 11240... Loss: 1.0269... Val Loss: 1.2532\n",
            "Epoch: 33/50... Step: 11250... Loss: 1.0449... Val Loss: 1.2475\n",
            "Epoch: 33/50... Step: 11260... Loss: 0.9977... Val Loss: 1.2500\n",
            "Epoch: 33/50... Step: 11270... Loss: 1.0637... Val Loss: 1.2516\n",
            "Epoch: 33/50... Step: 11280... Loss: 1.0312... Val Loss: 1.2506\n",
            "Epoch: 33/50... Step: 11290... Loss: 1.0396... Val Loss: 1.2487\n",
            "Epoch: 33/50... Step: 11300... Loss: 1.0639... Val Loss: 1.2454\n",
            "Epoch: 33/50... Step: 11310... Loss: 1.0420... Val Loss: 1.2495\n",
            "Epoch: 33/50... Step: 11320... Loss: 1.0288... Val Loss: 1.2422\n",
            "Epoch: 33/50... Step: 11330... Loss: 1.0755... Val Loss: 1.2434\n",
            "Epoch: 33/50... Step: 11340... Loss: 1.0217... Val Loss: 1.2471\n",
            "Epoch: 33/50... Step: 11350... Loss: 1.0436... Val Loss: 1.2461\n",
            "Epoch: 33/50... Step: 11360... Loss: 1.0457... Val Loss: 1.2504\n",
            "Epoch: 33/50... Step: 11370... Loss: 1.0186... Val Loss: 1.2406\n",
            "Epoch: 33/50... Step: 11380... Loss: 1.0706... Val Loss: 1.2517\n",
            "Epoch: 33/50... Step: 11390... Loss: 1.0708... Val Loss: 1.2464\n",
            "Epoch: 33/50... Step: 11400... Loss: 1.0561... Val Loss: 1.2418\n",
            "Epoch: 33/50... Step: 11410... Loss: 1.0595... Val Loss: 1.2461\n",
            "Epoch: 33/50... Step: 11420... Loss: 1.0528... Val Loss: 1.2516\n",
            "Epoch: 33/50... Step: 11430... Loss: 1.0187... Val Loss: 1.2427\n",
            "Epoch: 33/50... Step: 11440... Loss: 1.0651... Val Loss: 1.2505\n",
            "Epoch: 33/50... Step: 11450... Loss: 1.0675... Val Loss: 1.2425\n",
            "Epoch: 33/50... Step: 11460... Loss: 1.0641... Val Loss: 1.2452\n",
            "Epoch: 33/50... Step: 11470... Loss: 1.0923... Val Loss: 1.2464\n",
            "Epoch: 33/50... Step: 11480... Loss: 1.0175... Val Loss: 1.2406\n",
            "Epoch: 34/50... Step: 11490... Loss: 1.0513... Val Loss: 1.2494\n",
            "Epoch: 34/50... Step: 11500... Loss: 1.0431... Val Loss: 1.2542\n",
            "Epoch: 34/50... Step: 11510... Loss: 1.0696... Val Loss: 1.2456\n",
            "Epoch: 34/50... Step: 11520... Loss: 1.0567... Val Loss: 1.2549\n",
            "Epoch: 34/50... Step: 11530... Loss: 1.0574... Val Loss: 1.2503\n",
            "Epoch: 34/50... Step: 11540... Loss: 1.0488... Val Loss: 1.2543\n",
            "Epoch: 34/50... Step: 11550... Loss: 1.0330... Val Loss: 1.2511\n",
            "Epoch: 34/50... Step: 11560... Loss: 1.0540... Val Loss: 1.2569\n",
            "Epoch: 34/50... Step: 11570... Loss: 1.0176... Val Loss: 1.2499\n",
            "Epoch: 34/50... Step: 11580... Loss: 1.0420... Val Loss: 1.2527\n",
            "Epoch: 34/50... Step: 11590... Loss: 0.9994... Val Loss: 1.2548\n",
            "Epoch: 34/50... Step: 11600... Loss: 1.0257... Val Loss: 1.2493\n",
            "Epoch: 34/50... Step: 11610... Loss: 1.0222... Val Loss: 1.2539\n",
            "Epoch: 34/50... Step: 11620... Loss: 1.0602... Val Loss: 1.2496\n",
            "Epoch: 34/50... Step: 11630... Loss: 1.0045... Val Loss: 1.2535\n",
            "Epoch: 34/50... Step: 11640... Loss: 1.0354... Val Loss: 1.2529\n",
            "Epoch: 34/50... Step: 11650... Loss: 1.0567... Val Loss: 1.2502\n",
            "Epoch: 34/50... Step: 11660... Loss: 1.0592... Val Loss: 1.2510\n",
            "Epoch: 34/50... Step: 11670... Loss: 1.0565... Val Loss: 1.2472\n",
            "Epoch: 34/50... Step: 11680... Loss: 1.0619... Val Loss: 1.2481\n",
            "Epoch: 34/50... Step: 11690... Loss: 1.0178... Val Loss: 1.2485\n",
            "Epoch: 34/50... Step: 11700... Loss: 1.0559... Val Loss: 1.2463\n",
            "Epoch: 34/50... Step: 11710... Loss: 1.0740... Val Loss: 1.2552\n",
            "Epoch: 34/50... Step: 11720... Loss: 1.0674... Val Loss: 1.2457\n",
            "Epoch: 34/50... Step: 11730... Loss: 1.0679... Val Loss: 1.2501\n",
            "Epoch: 34/50... Step: 11740... Loss: 1.0315... Val Loss: 1.2534\n",
            "Epoch: 34/50... Step: 11750... Loss: 1.0400... Val Loss: 1.2472\n",
            "Epoch: 34/50... Step: 11760... Loss: 1.0412... Val Loss: 1.2497\n",
            "Epoch: 34/50... Step: 11770... Loss: 1.0420... Val Loss: 1.2491\n",
            "Epoch: 34/50... Step: 11780... Loss: 1.0230... Val Loss: 1.2478\n",
            "Epoch: 34/50... Step: 11790... Loss: 1.0447... Val Loss: 1.2480\n",
            "Epoch: 34/50... Step: 11800... Loss: 1.0426... Val Loss: 1.2472\n",
            "Epoch: 34/50... Step: 11810... Loss: 1.0676... Val Loss: 1.2498\n",
            "Epoch: 34/50... Step: 11820... Loss: 1.0697... Val Loss: 1.2474\n",
            "Epoch: 34/50... Step: 11830... Loss: 1.0346... Val Loss: 1.2515\n",
            "Epoch: 35/50... Step: 11840... Loss: 1.0395... Val Loss: 1.2465\n",
            "Epoch: 35/50... Step: 11850... Loss: 1.0400... Val Loss: 1.2563\n",
            "Epoch: 35/50... Step: 11860... Loss: 1.0428... Val Loss: 1.2462\n",
            "Epoch: 35/50... Step: 11870... Loss: 1.0367... Val Loss: 1.2553\n",
            "Epoch: 35/50... Step: 11880... Loss: 1.0686... Val Loss: 1.2519\n",
            "Epoch: 35/50... Step: 11890... Loss: 1.0205... Val Loss: 1.2508\n",
            "Epoch: 35/50... Step: 11900... Loss: 0.9897... Val Loss: 1.2476\n",
            "Epoch: 35/50... Step: 11910... Loss: 1.0267... Val Loss: 1.2554\n",
            "Epoch: 35/50... Step: 11920... Loss: 1.0223... Val Loss: 1.2500\n",
            "Epoch: 35/50... Step: 11930... Loss: 1.0564... Val Loss: 1.2582\n",
            "Epoch: 35/50... Step: 11940... Loss: 1.0470... Val Loss: 1.2520\n",
            "Epoch: 35/50... Step: 11950... Loss: 1.0579... Val Loss: 1.2561\n",
            "Epoch: 35/50... Step: 11960... Loss: 1.0167... Val Loss: 1.2549\n",
            "Epoch: 35/50... Step: 11970... Loss: 1.0528... Val Loss: 1.2549\n",
            "Epoch: 35/50... Step: 11980... Loss: 1.0204... Val Loss: 1.2535\n",
            "Epoch: 35/50... Step: 11990... Loss: 1.0121... Val Loss: 1.2516\n",
            "Epoch: 35/50... Step: 12000... Loss: 1.0290... Val Loss: 1.2550\n",
            "Epoch: 35/50... Step: 12010... Loss: 1.0345... Val Loss: 1.2505\n",
            "Epoch: 35/50... Step: 12020... Loss: 1.0352... Val Loss: 1.2533\n",
            "Epoch: 35/50... Step: 12030... Loss: 0.9917... Val Loss: 1.2479\n",
            "Epoch: 35/50... Step: 12040... Loss: 1.0409... Val Loss: 1.2492\n",
            "Epoch: 35/50... Step: 12050... Loss: 1.0508... Val Loss: 1.2501\n",
            "Epoch: 35/50... Step: 12060... Loss: 1.0849... Val Loss: 1.2564\n",
            "Epoch: 35/50... Step: 12070... Loss: 1.0395... Val Loss: 1.2550\n",
            "Epoch: 35/50... Step: 12080... Loss: 1.0603... Val Loss: 1.2465\n",
            "Epoch: 35/50... Step: 12090... Loss: 1.0618... Val Loss: 1.2539\n",
            "Epoch: 35/50... Step: 12100... Loss: 1.0548... Val Loss: 1.2495\n",
            "Epoch: 35/50... Step: 12110... Loss: 1.0031... Val Loss: 1.2548\n",
            "Epoch: 35/50... Step: 12120... Loss: 1.0911... Val Loss: 1.2535\n",
            "Epoch: 35/50... Step: 12130... Loss: 1.0427... Val Loss: 1.2492\n",
            "Epoch: 35/50... Step: 12140... Loss: 1.0063... Val Loss: 1.2516\n",
            "Epoch: 35/50... Step: 12150... Loss: 1.0483... Val Loss: 1.2505\n",
            "Epoch: 35/50... Step: 12160... Loss: 1.0607... Val Loss: 1.2519\n",
            "Epoch: 35/50... Step: 12170... Loss: 1.0493... Val Loss: 1.2500\n",
            "Epoch: 35/50... Step: 12180... Loss: 1.0886... Val Loss: 1.2559\n",
            "Epoch: 36/50... Step: 12190... Loss: 1.0500... Val Loss: 1.2601\n",
            "Epoch: 36/50... Step: 12200... Loss: 1.0652... Val Loss: 1.2564\n",
            "Epoch: 36/50... Step: 12210... Loss: 1.0574... Val Loss: 1.2533\n",
            "Epoch: 36/50... Step: 12220... Loss: 1.0473... Val Loss: 1.2540\n",
            "Epoch: 36/50... Step: 12230... Loss: 1.0468... Val Loss: 1.2540\n",
            "Epoch: 36/50... Step: 12240... Loss: 1.0386... Val Loss: 1.2603\n",
            "Epoch: 36/50... Step: 12250... Loss: 1.0241... Val Loss: 1.2496\n",
            "Epoch: 36/50... Step: 12260... Loss: 1.0540... Val Loss: 1.2571\n",
            "Epoch: 36/50... Step: 12270... Loss: 1.0445... Val Loss: 1.2489\n",
            "Epoch: 36/50... Step: 12280... Loss: 1.0279... Val Loss: 1.2583\n",
            "Epoch: 36/50... Step: 12290... Loss: 1.0218... Val Loss: 1.2580\n",
            "Epoch: 36/50... Step: 12300... Loss: 1.0447... Val Loss: 1.2536\n",
            "Epoch: 36/50... Step: 12310... Loss: 1.0471... Val Loss: 1.2549\n",
            "Epoch: 36/50... Step: 12320... Loss: 0.9943... Val Loss: 1.2565\n",
            "Epoch: 36/50... Step: 12330... Loss: 1.0070... Val Loss: 1.2568\n",
            "Epoch: 36/50... Step: 12340... Loss: 1.0362... Val Loss: 1.2492\n",
            "Epoch: 36/50... Step: 12350... Loss: 1.0102... Val Loss: 1.2554\n",
            "Epoch: 36/50... Step: 12360... Loss: 1.0307... Val Loss: 1.2482\n",
            "Epoch: 36/50... Step: 12370... Loss: 1.0771... Val Loss: 1.2525\n",
            "Epoch: 36/50... Step: 12380... Loss: 1.0664... Val Loss: 1.2485\n",
            "Epoch: 36/50... Step: 12390... Loss: 1.0356... Val Loss: 1.2532\n",
            "Epoch: 36/50... Step: 12400... Loss: 1.0946... Val Loss: 1.2506\n",
            "Epoch: 36/50... Step: 12410... Loss: 1.0325... Val Loss: 1.2544\n",
            "Epoch: 36/50... Step: 12420... Loss: 1.0408... Val Loss: 1.2498\n",
            "Epoch: 36/50... Step: 12430... Loss: 1.0152... Val Loss: 1.2453\n",
            "Epoch: 36/50... Step: 12440... Loss: 1.0311... Val Loss: 1.2579\n",
            "Epoch: 36/50... Step: 12450... Loss: 1.0348... Val Loss: 1.2484\n",
            "Epoch: 36/50... Step: 12460... Loss: 1.0518... Val Loss: 1.2518\n",
            "Epoch: 36/50... Step: 12470... Loss: 1.0186... Val Loss: 1.2562\n",
            "Epoch: 36/50... Step: 12480... Loss: 1.0472... Val Loss: 1.2495\n",
            "Epoch: 36/50... Step: 12490... Loss: 1.0469... Val Loss: 1.2445\n",
            "Epoch: 36/50... Step: 12500... Loss: 1.0234... Val Loss: 1.2541\n",
            "Epoch: 36/50... Step: 12510... Loss: 1.0141... Val Loss: 1.2507\n",
            "Epoch: 36/50... Step: 12520... Loss: 1.0662... Val Loss: 1.2477\n",
            "Epoch: 37/50... Step: 12530... Loss: 1.0505... Val Loss: 1.2590\n",
            "Epoch: 37/50... Step: 12540... Loss: 1.0239... Val Loss: 1.2528\n",
            "Epoch: 37/50... Step: 12550... Loss: 1.0830... Val Loss: 1.2534\n",
            "Epoch: 37/50... Step: 12560... Loss: 1.0433... Val Loss: 1.2508\n",
            "Epoch: 37/50... Step: 12570... Loss: 1.0368... Val Loss: 1.2562\n",
            "Epoch: 37/50... Step: 12580... Loss: 1.0124... Val Loss: 1.2504\n",
            "Epoch: 37/50... Step: 12590... Loss: 1.0200... Val Loss: 1.2563\n",
            "Epoch: 37/50... Step: 12600... Loss: 0.9951... Val Loss: 1.2553\n",
            "Epoch: 37/50... Step: 12610... Loss: 1.0407... Val Loss: 1.2572\n",
            "Epoch: 37/50... Step: 12620... Loss: 1.0567... Val Loss: 1.2563\n",
            "Epoch: 37/50... Step: 12630... Loss: 1.0147... Val Loss: 1.2548\n",
            "Epoch: 37/50... Step: 12640... Loss: 1.0480... Val Loss: 1.2570\n",
            "Epoch: 37/50... Step: 12650... Loss: 1.0177... Val Loss: 1.2534\n",
            "Epoch: 37/50... Step: 12660... Loss: 0.9747... Val Loss: 1.2523\n",
            "Epoch: 37/50... Step: 12670... Loss: 0.9968... Val Loss: 1.2552\n",
            "Epoch: 37/50... Step: 12680... Loss: 1.0051... Val Loss: 1.2479\n",
            "Epoch: 37/50... Step: 12690... Loss: 1.0227... Val Loss: 1.2497\n",
            "Epoch: 37/50... Step: 12700... Loss: 1.0073... Val Loss: 1.2491\n",
            "Epoch: 37/50... Step: 12710... Loss: 1.0614... Val Loss: 1.2461\n",
            "Epoch: 37/50... Step: 12720... Loss: 1.0360... Val Loss: 1.2539\n",
            "Epoch: 37/50... Step: 12730... Loss: 1.0359... Val Loss: 1.2444\n",
            "Epoch: 37/50... Step: 12740... Loss: 1.0200... Val Loss: 1.2507\n",
            "Epoch: 37/50... Step: 12750... Loss: 1.0392... Val Loss: 1.2528\n",
            "Epoch: 37/50... Step: 12760... Loss: 1.0421... Val Loss: 1.2563\n",
            "Epoch: 37/50... Step: 12770... Loss: 1.0279... Val Loss: 1.2513\n",
            "Epoch: 37/50... Step: 12780... Loss: 1.0207... Val Loss: 1.2498\n",
            "Epoch: 37/50... Step: 12790... Loss: 1.0581... Val Loss: 1.2617\n",
            "Epoch: 37/50... Step: 12800... Loss: 1.0644... Val Loss: 1.2502\n",
            "Epoch: 37/50... Step: 12810... Loss: 1.0198... Val Loss: 1.2523\n",
            "Epoch: 37/50... Step: 12820... Loss: 1.0313... Val Loss: 1.2554\n",
            "Epoch: 37/50... Step: 12830... Loss: 1.0329... Val Loss: 1.2509\n",
            "Epoch: 37/50... Step: 12840... Loss: 1.0222... Val Loss: 1.2514\n",
            "Epoch: 37/50... Step: 12850... Loss: 1.0573... Val Loss: 1.2582\n",
            "Epoch: 37/50... Step: 12860... Loss: 1.0537... Val Loss: 1.2538\n",
            "Epoch: 37/50... Step: 12870... Loss: 1.0241... Val Loss: 1.2506\n",
            "Epoch: 38/50... Step: 12880... Loss: 1.0138... Val Loss: 1.2648\n",
            "Epoch: 38/50... Step: 12890... Loss: 1.0724... Val Loss: 1.2556\n",
            "Epoch: 38/50... Step: 12900... Loss: 1.0120... Val Loss: 1.2570\n",
            "Epoch: 38/50... Step: 12910... Loss: 1.0128... Val Loss: 1.2589\n",
            "Epoch: 38/50... Step: 12920... Loss: 1.0229... Val Loss: 1.2569\n",
            "Epoch: 38/50... Step: 12930... Loss: 1.0103... Val Loss: 1.2551\n",
            "Epoch: 38/50... Step: 12940... Loss: 1.0211... Val Loss: 1.2548\n",
            "Epoch: 38/50... Step: 12950... Loss: 1.0367... Val Loss: 1.2582\n",
            "Epoch: 38/50... Step: 12960... Loss: 1.0075... Val Loss: 1.2519\n",
            "Epoch: 38/50... Step: 12970... Loss: 0.9911... Val Loss: 1.2606\n",
            "Epoch: 38/50... Step: 12980... Loss: 1.0214... Val Loss: 1.2547\n",
            "Epoch: 38/50... Step: 12990... Loss: 1.0261... Val Loss: 1.2560\n",
            "Epoch: 38/50... Step: 13000... Loss: 0.9852... Val Loss: 1.2551\n",
            "Epoch: 38/50... Step: 13010... Loss: 1.0294... Val Loss: 1.2579\n",
            "Epoch: 38/50... Step: 13020... Loss: 1.0312... Val Loss: 1.2570\n",
            "Epoch: 38/50... Step: 13030... Loss: 1.0138... Val Loss: 1.2606\n",
            "Epoch: 38/50... Step: 13040... Loss: 1.0377... Val Loss: 1.2585\n",
            "Epoch: 38/50... Step: 13050... Loss: 1.0068... Val Loss: 1.2533\n",
            "Epoch: 38/50... Step: 13060... Loss: 1.0050... Val Loss: 1.2487\n",
            "Epoch: 38/50... Step: 13070... Loss: 1.0625... Val Loss: 1.2556\n",
            "Epoch: 38/50... Step: 13080... Loss: 1.0011... Val Loss: 1.2500\n",
            "Epoch: 38/50... Step: 13090... Loss: 1.0159... Val Loss: 1.2532\n",
            "Epoch: 38/50... Step: 13100... Loss: 1.0250... Val Loss: 1.2578\n",
            "Epoch: 38/50... Step: 13110... Loss: 0.9917... Val Loss: 1.2571\n",
            "Epoch: 38/50... Step: 13120... Loss: 1.0480... Val Loss: 1.2593\n",
            "Epoch: 38/50... Step: 13130... Loss: 1.0435... Val Loss: 1.2606\n",
            "Epoch: 38/50... Step: 13140... Loss: 1.0310... Val Loss: 1.2606\n",
            "Epoch: 38/50... Step: 13150... Loss: 1.0373... Val Loss: 1.2553\n",
            "Epoch: 38/50... Step: 13160... Loss: 1.0321... Val Loss: 1.2541\n",
            "Epoch: 38/50... Step: 13170... Loss: 0.9956... Val Loss: 1.2511\n",
            "Epoch: 38/50... Step: 13180... Loss: 1.0396... Val Loss: 1.2557\n",
            "Epoch: 38/50... Step: 13190... Loss: 1.0569... Val Loss: 1.2547\n",
            "Epoch: 38/50... Step: 13200... Loss: 1.0335... Val Loss: 1.2602\n",
            "Epoch: 38/50... Step: 13210... Loss: 1.0493... Val Loss: 1.2591\n",
            "Epoch: 38/50... Step: 13220... Loss: 1.0081... Val Loss: 1.2537\n",
            "Epoch: 39/50... Step: 13230... Loss: 1.0366... Val Loss: 1.2591\n",
            "Epoch: 39/50... Step: 13240... Loss: 1.0195... Val Loss: 1.2564\n",
            "Epoch: 39/50... Step: 13250... Loss: 1.0494... Val Loss: 1.2571\n",
            "Epoch: 39/50... Step: 13260... Loss: 1.0477... Val Loss: 1.2601\n",
            "Epoch: 39/50... Step: 13270... Loss: 1.0363... Val Loss: 1.2567\n",
            "Epoch: 39/50... Step: 13280... Loss: 1.0178... Val Loss: 1.2586\n",
            "Epoch: 39/50... Step: 13290... Loss: 1.0075... Val Loss: 1.2563\n",
            "Epoch: 39/50... Step: 13300... Loss: 1.0389... Val Loss: 1.2591\n",
            "Epoch: 39/50... Step: 13310... Loss: 0.9971... Val Loss: 1.2584\n",
            "Epoch: 39/50... Step: 13320... Loss: 1.0137... Val Loss: 1.2611\n",
            "Epoch: 39/50... Step: 13330... Loss: 1.0049... Val Loss: 1.2619\n",
            "Epoch: 39/50... Step: 13340... Loss: 1.0081... Val Loss: 1.2570\n",
            "Epoch: 39/50... Step: 13350... Loss: 0.9984... Val Loss: 1.2600\n",
            "Epoch: 39/50... Step: 13360... Loss: 1.0495... Val Loss: 1.2623\n",
            "Epoch: 39/50... Step: 13370... Loss: 1.0000... Val Loss: 1.2584\n",
            "Epoch: 39/50... Step: 13380... Loss: 1.0203... Val Loss: 1.2620\n",
            "Epoch: 39/50... Step: 13390... Loss: 1.0301... Val Loss: 1.2611\n",
            "Epoch: 39/50... Step: 13400... Loss: 1.0419... Val Loss: 1.2598\n",
            "Epoch: 39/50... Step: 13410... Loss: 1.0190... Val Loss: 1.2537\n",
            "Epoch: 39/50... Step: 13420... Loss: 1.0605... Val Loss: 1.2652\n",
            "Epoch: 39/50... Step: 13430... Loss: 0.9965... Val Loss: 1.2571\n",
            "Epoch: 39/50... Step: 13440... Loss: 1.0465... Val Loss: 1.2587\n",
            "Epoch: 39/50... Step: 13450... Loss: 1.0556... Val Loss: 1.2595\n",
            "Epoch: 39/50... Step: 13460... Loss: 1.0358... Val Loss: 1.2601\n",
            "Epoch: 39/50... Step: 13470... Loss: 1.0342... Val Loss: 1.2633\n",
            "Epoch: 39/50... Step: 13480... Loss: 1.0186... Val Loss: 1.2551\n",
            "Epoch: 39/50... Step: 13490... Loss: 1.0229... Val Loss: 1.2596\n",
            "Epoch: 39/50... Step: 13500... Loss: 1.0286... Val Loss: 1.2549\n",
            "Epoch: 39/50... Step: 13510... Loss: 1.0339... Val Loss: 1.2588\n",
            "Epoch: 39/50... Step: 13520... Loss: 0.9889... Val Loss: 1.2558\n",
            "Epoch: 39/50... Step: 13530... Loss: 1.0310... Val Loss: 1.2611\n",
            "Epoch: 39/50... Step: 13540... Loss: 1.0220... Val Loss: 1.2596\n",
            "Epoch: 39/50... Step: 13550... Loss: 1.0500... Val Loss: 1.2548\n",
            "Epoch: 39/50... Step: 13560... Loss: 1.0455... Val Loss: 1.2594\n",
            "Epoch: 39/50... Step: 13570... Loss: 1.0112... Val Loss: 1.2589\n",
            "Epoch: 40/50... Step: 13580... Loss: 1.0060... Val Loss: 1.2562\n",
            "Epoch: 40/50... Step: 13590... Loss: 1.0288... Val Loss: 1.2552\n",
            "Epoch: 40/50... Step: 13600... Loss: 1.0217... Val Loss: 1.2587\n",
            "Epoch: 40/50... Step: 13610... Loss: 1.0300... Val Loss: 1.2618\n",
            "Epoch: 40/50... Step: 13620... Loss: 1.0515... Val Loss: 1.2635\n",
            "Epoch: 40/50... Step: 13630... Loss: 1.0071... Val Loss: 1.2615\n",
            "Epoch: 40/50... Step: 13640... Loss: 0.9878... Val Loss: 1.2577\n",
            "Epoch: 40/50... Step: 13650... Loss: 1.0013... Val Loss: 1.2676\n",
            "Epoch: 40/50... Step: 13660... Loss: 1.0059... Val Loss: 1.2637\n",
            "Epoch: 40/50... Step: 13670... Loss: 1.0316... Val Loss: 1.2670\n",
            "Epoch: 40/50... Step: 13680... Loss: 1.0177... Val Loss: 1.2614\n",
            "Epoch: 40/50... Step: 13690... Loss: 1.0471... Val Loss: 1.2570\n",
            "Epoch: 40/50... Step: 13700... Loss: 1.0078... Val Loss: 1.2601\n",
            "Epoch: 40/50... Step: 13710... Loss: 1.0279... Val Loss: 1.2574\n",
            "Epoch: 40/50... Step: 13720... Loss: 1.0140... Val Loss: 1.2575\n",
            "Epoch: 40/50... Step: 13730... Loss: 1.0132... Val Loss: 1.2623\n",
            "Epoch: 40/50... Step: 13740... Loss: 1.0218... Val Loss: 1.2602\n",
            "Epoch: 40/50... Step: 13750... Loss: 1.0142... Val Loss: 1.2530\n",
            "Epoch: 40/50... Step: 13760... Loss: 1.0160... Val Loss: 1.2531\n",
            "Epoch: 40/50... Step: 13770... Loss: 0.9756... Val Loss: 1.2563\n",
            "Epoch: 40/50... Step: 13780... Loss: 1.0263... Val Loss: 1.2622\n",
            "Epoch: 40/50... Step: 13790... Loss: 1.0502... Val Loss: 1.2578\n",
            "Epoch: 40/50... Step: 13800... Loss: 1.0687... Val Loss: 1.2643\n",
            "Epoch: 40/50... Step: 13810... Loss: 1.0233... Val Loss: 1.2558\n",
            "Epoch: 40/50... Step: 13820... Loss: 1.0365... Val Loss: 1.2569\n",
            "Epoch: 40/50... Step: 13830... Loss: 1.0444... Val Loss: 1.2563\n",
            "Epoch: 40/50... Step: 13840... Loss: 1.0431... Val Loss: 1.2527\n",
            "Epoch: 40/50... Step: 13850... Loss: 0.9735... Val Loss: 1.2567\n",
            "Epoch: 40/50... Step: 13860... Loss: 1.0816... Val Loss: 1.2575\n",
            "Epoch: 40/50... Step: 13870... Loss: 1.0102... Val Loss: 1.2540\n",
            "Epoch: 40/50... Step: 13880... Loss: 0.9809... Val Loss: 1.2566\n",
            "Epoch: 40/50... Step: 13890... Loss: 1.0372... Val Loss: 1.2553\n",
            "Epoch: 40/50... Step: 13900... Loss: 1.0387... Val Loss: 1.2599\n",
            "Epoch: 40/50... Step: 13910... Loss: 1.0363... Val Loss: 1.2574\n",
            "Epoch: 40/50... Step: 13920... Loss: 1.0864... Val Loss: 1.2618\n",
            "Epoch: 41/50... Step: 13930... Loss: 1.0260... Val Loss: 1.2619\n",
            "Epoch: 41/50... Step: 13940... Loss: 1.0411... Val Loss: 1.2612\n",
            "Epoch: 41/50... Step: 13950... Loss: 1.0246... Val Loss: 1.2590\n",
            "Epoch: 41/50... Step: 13960... Loss: 1.0144... Val Loss: 1.2643\n",
            "Epoch: 41/50... Step: 13970... Loss: 1.0167... Val Loss: 1.2637\n",
            "Epoch: 41/50... Step: 13980... Loss: 1.0341... Val Loss: 1.2606\n",
            "Epoch: 41/50... Step: 13990... Loss: 0.9891... Val Loss: 1.2555\n",
            "Epoch: 41/50... Step: 14000... Loss: 1.0297... Val Loss: 1.2684\n",
            "Epoch: 41/50... Step: 14010... Loss: 1.0182... Val Loss: 1.2627\n",
            "Epoch: 41/50... Step: 14020... Loss: 1.0138... Val Loss: 1.2691\n",
            "Epoch: 41/50... Step: 14030... Loss: 1.0031... Val Loss: 1.2661\n",
            "Epoch: 41/50... Step: 14040... Loss: 1.0185... Val Loss: 1.2676\n",
            "Epoch: 41/50... Step: 14050... Loss: 1.0310... Val Loss: 1.2606\n",
            "Epoch: 41/50... Step: 14060... Loss: 0.9821... Val Loss: 1.2616\n",
            "Epoch: 41/50... Step: 14070... Loss: 0.9847... Val Loss: 1.2600\n",
            "Epoch: 41/50... Step: 14080... Loss: 1.0329... Val Loss: 1.2651\n",
            "Epoch: 41/50... Step: 14090... Loss: 0.9990... Val Loss: 1.2605\n",
            "Epoch: 41/50... Step: 14100... Loss: 1.0081... Val Loss: 1.2594\n",
            "Epoch: 41/50... Step: 14110... Loss: 1.0529... Val Loss: 1.2570\n",
            "Epoch: 41/50... Step: 14120... Loss: 1.0733... Val Loss: 1.2590\n",
            "Epoch: 41/50... Step: 14130... Loss: 1.0144... Val Loss: 1.2614\n",
            "Epoch: 41/50... Step: 14140... Loss: 1.0814... Val Loss: 1.2598\n",
            "Epoch: 41/50... Step: 14150... Loss: 1.0121... Val Loss: 1.2641\n",
            "Epoch: 41/50... Step: 14160... Loss: 1.0154... Val Loss: 1.2598\n",
            "Epoch: 41/50... Step: 14170... Loss: 0.9981... Val Loss: 1.2627\n",
            "Epoch: 41/50... Step: 14180... Loss: 1.0157... Val Loss: 1.2598\n",
            "Epoch: 41/50... Step: 14190... Loss: 1.0248... Val Loss: 1.2602\n",
            "Epoch: 41/50... Step: 14200... Loss: 1.0299... Val Loss: 1.2604\n",
            "Epoch: 41/50... Step: 14210... Loss: 0.9837... Val Loss: 1.2572\n",
            "Epoch: 41/50... Step: 14220... Loss: 1.0241... Val Loss: 1.2554\n",
            "Epoch: 41/50... Step: 14230... Loss: 1.0238... Val Loss: 1.2562\n",
            "Epoch: 41/50... Step: 14240... Loss: 1.0305... Val Loss: 1.2597\n",
            "Epoch: 41/50... Step: 14250... Loss: 1.0139... Val Loss: 1.2534\n",
            "Epoch: 41/50... Step: 14260... Loss: 1.0407... Val Loss: 1.2560\n",
            "Epoch: 42/50... Step: 14270... Loss: 1.0549... Val Loss: 1.2621\n",
            "Epoch: 42/50... Step: 14280... Loss: 1.0190... Val Loss: 1.2690\n",
            "Epoch: 42/50... Step: 14290... Loss: 1.0594... Val Loss: 1.2604\n",
            "Epoch: 42/50... Step: 14300... Loss: 1.0144... Val Loss: 1.2563\n",
            "Epoch: 42/50... Step: 14310... Loss: 1.0180... Val Loss: 1.2591\n",
            "Epoch: 42/50... Step: 14320... Loss: 0.9959... Val Loss: 1.2591\n",
            "Epoch: 42/50... Step: 14330... Loss: 0.9938... Val Loss: 1.2629\n",
            "Epoch: 42/50... Step: 14340... Loss: 0.9908... Val Loss: 1.2569\n",
            "Epoch: 42/50... Step: 14350... Loss: 1.0219... Val Loss: 1.2677\n",
            "Epoch: 42/50... Step: 14360... Loss: 1.0388... Val Loss: 1.2578\n",
            "Epoch: 42/50... Step: 14370... Loss: 0.9913... Val Loss: 1.2677\n",
            "Epoch: 42/50... Step: 14380... Loss: 1.0281... Val Loss: 1.2589\n",
            "Epoch: 42/50... Step: 14390... Loss: 1.0152... Val Loss: 1.2641\n",
            "Epoch: 42/50... Step: 14400... Loss: 0.9544... Val Loss: 1.2603\n",
            "Epoch: 42/50... Step: 14410... Loss: 0.9769... Val Loss: 1.2580\n",
            "Epoch: 42/50... Step: 14420... Loss: 0.9949... Val Loss: 1.2634\n",
            "Epoch: 42/50... Step: 14430... Loss: 1.0005... Val Loss: 1.2528\n",
            "Epoch: 42/50... Step: 14440... Loss: 0.9859... Val Loss: 1.2556\n",
            "Epoch: 42/50... Step: 14450... Loss: 1.0374... Val Loss: 1.2541\n",
            "Epoch: 42/50... Step: 14460... Loss: 1.0277... Val Loss: 1.2612\n",
            "Epoch: 42/50... Step: 14470... Loss: 1.0223... Val Loss: 1.2658\n",
            "Epoch: 42/50... Step: 14480... Loss: 1.0156... Val Loss: 1.2580\n",
            "Epoch: 42/50... Step: 14490... Loss: 1.0251... Val Loss: 1.2612\n",
            "Epoch: 42/50... Step: 14500... Loss: 1.0165... Val Loss: 1.2630\n",
            "Epoch: 42/50... Step: 14510... Loss: 1.0103... Val Loss: 1.2592\n",
            "Epoch: 42/50... Step: 14520... Loss: 1.0109... Val Loss: 1.2579\n",
            "Epoch: 42/50... Step: 14530... Loss: 1.0190... Val Loss: 1.2637\n",
            "Epoch: 42/50... Step: 14540... Loss: 1.0429... Val Loss: 1.2586\n",
            "Epoch: 42/50... Step: 14550... Loss: 1.0139... Val Loss: 1.2581\n",
            "Epoch: 42/50... Step: 14560... Loss: 1.0188... Val Loss: 1.2646\n",
            "Epoch: 42/50... Step: 14570... Loss: 1.0046... Val Loss: 1.2563\n",
            "Epoch: 42/50... Step: 14580... Loss: 1.0118... Val Loss: 1.2571\n",
            "Epoch: 42/50... Step: 14590... Loss: 1.0391... Val Loss: 1.2590\n",
            "Epoch: 42/50... Step: 14600... Loss: 1.0489... Val Loss: 1.2562\n",
            "Epoch: 42/50... Step: 14610... Loss: 1.0073... Val Loss: 1.2560\n",
            "Epoch: 43/50... Step: 14620... Loss: 1.0073... Val Loss: 1.2671\n",
            "Epoch: 43/50... Step: 14630... Loss: 1.0515... Val Loss: 1.2555\n",
            "Epoch: 43/50... Step: 14640... Loss: 1.0077... Val Loss: 1.2621\n",
            "Epoch: 43/50... Step: 14650... Loss: 0.9864... Val Loss: 1.2613\n",
            "Epoch: 43/50... Step: 14660... Loss: 1.0250... Val Loss: 1.2617\n",
            "Epoch: 43/50... Step: 14670... Loss: 0.9804... Val Loss: 1.2637\n",
            "Epoch: 43/50... Step: 14680... Loss: 1.0112... Val Loss: 1.2650\n",
            "Epoch: 43/50... Step: 14690... Loss: 1.0186... Val Loss: 1.2605\n",
            "Epoch: 43/50... Step: 14700... Loss: 1.0005... Val Loss: 1.2600\n",
            "Epoch: 43/50... Step: 14710... Loss: 0.9623... Val Loss: 1.2615\n",
            "Epoch: 43/50... Step: 14720... Loss: 0.9987... Val Loss: 1.2659\n",
            "Epoch: 43/50... Step: 14730... Loss: 1.0132... Val Loss: 1.2590\n",
            "Epoch: 43/50... Step: 14740... Loss: 0.9610... Val Loss: 1.2617\n",
            "Epoch: 43/50... Step: 14750... Loss: 1.0137... Val Loss: 1.2581\n",
            "Epoch: 43/50... Step: 14760... Loss: 1.0218... Val Loss: 1.2585\n",
            "Epoch: 43/50... Step: 14770... Loss: 1.0006... Val Loss: 1.2608\n",
            "Epoch: 43/50... Step: 14780... Loss: 1.0309... Val Loss: 1.2569\n",
            "Epoch: 43/50... Step: 14790... Loss: 0.9884... Val Loss: 1.2581\n",
            "Epoch: 43/50... Step: 14800... Loss: 1.0046... Val Loss: 1.2594\n",
            "Epoch: 43/50... Step: 14810... Loss: 1.0324... Val Loss: 1.2601\n",
            "Epoch: 43/50... Step: 14820... Loss: 0.9828... Val Loss: 1.2589\n",
            "Epoch: 43/50... Step: 14830... Loss: 1.0005... Val Loss: 1.2644\n",
            "Epoch: 43/50... Step: 14840... Loss: 1.0242... Val Loss: 1.2622\n",
            "Epoch: 43/50... Step: 14850... Loss: 0.9875... Val Loss: 1.2604\n",
            "Epoch: 43/50... Step: 14860... Loss: 1.0280... Val Loss: 1.2594\n",
            "Epoch: 43/50... Step: 14870... Loss: 1.0187... Val Loss: 1.2544\n",
            "Epoch: 43/50... Step: 14880... Loss: 1.0254... Val Loss: 1.2615\n",
            "Epoch: 43/50... Step: 14890... Loss: 1.0124... Val Loss: 1.2563\n",
            "Epoch: 43/50... Step: 14900... Loss: 1.0204... Val Loss: 1.2612\n",
            "Epoch: 43/50... Step: 14910... Loss: 0.9883... Val Loss: 1.2623\n",
            "Epoch: 43/50... Step: 14920... Loss: 1.0268... Val Loss: 1.2542\n",
            "Epoch: 43/50... Step: 14930... Loss: 1.0481... Val Loss: 1.2587\n",
            "Epoch: 43/50... Step: 14940... Loss: 1.0182... Val Loss: 1.2616\n",
            "Epoch: 43/50... Step: 14950... Loss: 1.0418... Val Loss: 1.2584\n",
            "Epoch: 43/50... Step: 14960... Loss: 0.9836... Val Loss: 1.2621\n",
            "Epoch: 44/50... Step: 14970... Loss: 1.0232... Val Loss: 1.2641\n",
            "Epoch: 44/50... Step: 14980... Loss: 1.0095... Val Loss: 1.2621\n",
            "Epoch: 44/50... Step: 14990... Loss: 1.0377... Val Loss: 1.2635\n",
            "Epoch: 44/50... Step: 15000... Loss: 1.0283... Val Loss: 1.2661\n",
            "Epoch: 44/50... Step: 15010... Loss: 1.0105... Val Loss: 1.2663\n",
            "Epoch: 44/50... Step: 15020... Loss: 1.0206... Val Loss: 1.2640\n",
            "Epoch: 44/50... Step: 15030... Loss: 0.9974... Val Loss: 1.2639\n",
            "Epoch: 44/50... Step: 15040... Loss: 1.0146... Val Loss: 1.2671\n",
            "Epoch: 44/50... Step: 15050... Loss: 0.9644... Val Loss: 1.2675\n",
            "Epoch: 44/50... Step: 15060... Loss: 0.9997... Val Loss: 1.2660\n",
            "Epoch: 44/50... Step: 15070... Loss: 0.9741... Val Loss: 1.2752\n",
            "Epoch: 44/50... Step: 15080... Loss: 0.9937... Val Loss: 1.2608\n",
            "Epoch: 44/50... Step: 15090... Loss: 0.9887... Val Loss: 1.2646\n",
            "Epoch: 44/50... Step: 15100... Loss: 1.0236... Val Loss: 1.2611\n",
            "Epoch: 44/50... Step: 15110... Loss: 0.9754... Val Loss: 1.2569\n",
            "Epoch: 44/50... Step: 15120... Loss: 1.0038... Val Loss: 1.2637\n",
            "Epoch: 44/50... Step: 15130... Loss: 1.0072... Val Loss: 1.2577\n",
            "Epoch: 44/50... Step: 15140... Loss: 1.0195... Val Loss: 1.2653\n",
            "Epoch: 44/50... Step: 15150... Loss: 1.0030... Val Loss: 1.2595\n",
            "Epoch: 44/50... Step: 15160... Loss: 1.0351... Val Loss: 1.2663\n",
            "Epoch: 44/50... Step: 15170... Loss: 0.9830... Val Loss: 1.2582\n",
            "Epoch: 44/50... Step: 15180... Loss: 1.0309... Val Loss: 1.2637\n",
            "Epoch: 44/50... Step: 15190... Loss: 1.0400... Val Loss: 1.2688\n",
            "Epoch: 44/50... Step: 15200... Loss: 1.0419... Val Loss: 1.2580\n",
            "Epoch: 44/50... Step: 15210... Loss: 1.0190... Val Loss: 1.2650\n",
            "Epoch: 44/50... Step: 15220... Loss: 1.0072... Val Loss: 1.2550\n",
            "Epoch: 44/50... Step: 15230... Loss: 1.0073... Val Loss: 1.2634\n",
            "Epoch: 44/50... Step: 15240... Loss: 1.0145... Val Loss: 1.2596\n",
            "Epoch: 44/50... Step: 15250... Loss: 1.0115... Val Loss: 1.2668\n",
            "Epoch: 44/50... Step: 15260... Loss: 0.9717... Val Loss: 1.2651\n",
            "Epoch: 44/50... Step: 15270... Loss: 1.0135... Val Loss: 1.2608\n",
            "Epoch: 44/50... Step: 15280... Loss: 1.0062... Val Loss: 1.2632\n",
            "Epoch: 44/50... Step: 15290... Loss: 1.0343... Val Loss: 1.2626\n",
            "Epoch: 44/50... Step: 15300... Loss: 1.0313... Val Loss: 1.2581\n",
            "Epoch: 44/50... Step: 15310... Loss: 0.9981... Val Loss: 1.2640\n",
            "Epoch: 45/50... Step: 15320... Loss: 1.0000... Val Loss: 1.2586\n",
            "Epoch: 45/50... Step: 15330... Loss: 1.0081... Val Loss: 1.2624\n",
            "Epoch: 45/50... Step: 15340... Loss: 1.0059... Val Loss: 1.2654\n",
            "Epoch: 45/50... Step: 15350... Loss: 1.0052... Val Loss: 1.2690\n",
            "Epoch: 45/50... Step: 15360... Loss: 1.0361... Val Loss: 1.2656\n",
            "Epoch: 45/50... Step: 15370... Loss: 0.9824... Val Loss: 1.2706\n",
            "Epoch: 45/50... Step: 15380... Loss: 0.9650... Val Loss: 1.2659\n",
            "Epoch: 45/50... Step: 15390... Loss: 0.9889... Val Loss: 1.2658\n",
            "Epoch: 45/50... Step: 15400... Loss: 0.9733... Val Loss: 1.2627\n",
            "Epoch: 45/50... Step: 15410... Loss: 1.0052... Val Loss: 1.2698\n",
            "Epoch: 45/50... Step: 15420... Loss: 1.0130... Val Loss: 1.2697\n",
            "Epoch: 45/50... Step: 15430... Loss: 1.0457... Val Loss: 1.2619\n",
            "Epoch: 45/50... Step: 15440... Loss: 0.9785... Val Loss: 1.2695\n",
            "Epoch: 45/50... Step: 15450... Loss: 1.0096... Val Loss: 1.2661\n",
            "Epoch: 45/50... Step: 15460... Loss: 0.9932... Val Loss: 1.2665\n",
            "Epoch: 45/50... Step: 15470... Loss: 0.9920... Val Loss: 1.2688\n",
            "Epoch: 45/50... Step: 15480... Loss: 0.9994... Val Loss: 1.2640\n",
            "Epoch: 45/50... Step: 15490... Loss: 1.0094... Val Loss: 1.2678\n",
            "Epoch: 45/50... Step: 15500... Loss: 0.9885... Val Loss: 1.2631\n",
            "Epoch: 45/50... Step: 15510... Loss: 0.9611... Val Loss: 1.2659\n",
            "Epoch: 45/50... Step: 15520... Loss: 0.9964... Val Loss: 1.2675\n",
            "Epoch: 45/50... Step: 15530... Loss: 1.0303... Val Loss: 1.2659\n",
            "Epoch: 45/50... Step: 15540... Loss: 1.0379... Val Loss: 1.2699\n",
            "Epoch: 45/50... Step: 15550... Loss: 1.0251... Val Loss: 1.2616\n",
            "Epoch: 45/50... Step: 15560... Loss: 1.0116... Val Loss: 1.2661\n",
            "Epoch: 45/50... Step: 15570... Loss: 1.0325... Val Loss: 1.2609\n",
            "Epoch: 45/50... Step: 15580... Loss: 1.0290... Val Loss: 1.2676\n",
            "Epoch: 45/50... Step: 15590... Loss: 0.9784... Val Loss: 1.2650\n",
            "Epoch: 45/50... Step: 15600... Loss: 1.0600... Val Loss: 1.2651\n",
            "Epoch: 45/50... Step: 15610... Loss: 1.0004... Val Loss: 1.2632\n",
            "Epoch: 45/50... Step: 15620... Loss: 0.9814... Val Loss: 1.2660\n",
            "Epoch: 45/50... Step: 15630... Loss: 1.0241... Val Loss: 1.2637\n",
            "Epoch: 45/50... Step: 15640... Loss: 1.0273... Val Loss: 1.2651\n",
            "Epoch: 45/50... Step: 15650... Loss: 1.0251... Val Loss: 1.2656\n",
            "Epoch: 45/50... Step: 15660... Loss: 1.0645... Val Loss: 1.2621\n",
            "Epoch: 46/50... Step: 15670... Loss: 1.0089... Val Loss: 1.2649\n",
            "Epoch: 46/50... Step: 15680... Loss: 1.0185... Val Loss: 1.2623\n",
            "Epoch: 46/50... Step: 15690... Loss: 1.0097... Val Loss: 1.2645\n",
            "Epoch: 46/50... Step: 15700... Loss: 1.0100... Val Loss: 1.2638\n",
            "Epoch: 46/50... Step: 15710... Loss: 1.0137... Val Loss: 1.2626\n",
            "Epoch: 46/50... Step: 15720... Loss: 1.0226... Val Loss: 1.2689\n",
            "Epoch: 46/50... Step: 15730... Loss: 0.9823... Val Loss: 1.2619\n",
            "Epoch: 46/50... Step: 15740... Loss: 1.0125... Val Loss: 1.2705\n",
            "Epoch: 46/50... Step: 15750... Loss: 1.0022... Val Loss: 1.2651\n",
            "Epoch: 46/50... Step: 15760... Loss: 0.9858... Val Loss: 1.2707\n",
            "Epoch: 46/50... Step: 15770... Loss: 0.9992... Val Loss: 1.2670\n",
            "Epoch: 46/50... Step: 15780... Loss: 1.0027... Val Loss: 1.2659\n",
            "Epoch: 46/50... Step: 15790... Loss: 1.0106... Val Loss: 1.2705\n",
            "Epoch: 46/50... Step: 15800... Loss: 0.9722... Val Loss: 1.2659\n",
            "Epoch: 46/50... Step: 15810... Loss: 0.9704... Val Loss: 1.2631\n",
            "Epoch: 46/50... Step: 15820... Loss: 1.0226... Val Loss: 1.2647\n",
            "Epoch: 46/50... Step: 15830... Loss: 0.9767... Val Loss: 1.2624\n",
            "Epoch: 46/50... Step: 15840... Loss: 0.9889... Val Loss: 1.2642\n",
            "Epoch: 46/50... Step: 15850... Loss: 1.0251... Val Loss: 1.2674\n",
            "Epoch: 46/50... Step: 15860... Loss: 1.0411... Val Loss: 1.2673\n",
            "Epoch: 46/50... Step: 15870... Loss: 0.9878... Val Loss: 1.2693\n",
            "Epoch: 46/50... Step: 15880... Loss: 1.0649... Val Loss: 1.2620\n",
            "Epoch: 46/50... Step: 15890... Loss: 1.0015... Val Loss: 1.2667\n",
            "Epoch: 46/50... Step: 15900... Loss: 1.0010... Val Loss: 1.2635\n",
            "Epoch: 46/50... Step: 15910... Loss: 0.9667... Val Loss: 1.2675\n",
            "Epoch: 46/50... Step: 15920... Loss: 0.9978... Val Loss: 1.2650\n",
            "Epoch: 46/50... Step: 15930... Loss: 1.0196... Val Loss: 1.2628\n",
            "Epoch: 46/50... Step: 15940... Loss: 1.0160... Val Loss: 1.2673\n",
            "Epoch: 46/50... Step: 15950... Loss: 0.9800... Val Loss: 1.2657\n",
            "Epoch: 46/50... Step: 15960... Loss: 0.9983... Val Loss: 1.2653\n",
            "Epoch: 46/50... Step: 15970... Loss: 1.0130... Val Loss: 1.2635\n",
            "Epoch: 46/50... Step: 15980... Loss: 0.9931... Val Loss: 1.2720\n",
            "Epoch: 46/50... Step: 15990... Loss: 0.9936... Val Loss: 1.2631\n",
            "Epoch: 46/50... Step: 16000... Loss: 1.0272... Val Loss: 1.2680\n",
            "Epoch: 47/50... Step: 16010... Loss: 1.0414... Val Loss: 1.2626\n",
            "Epoch: 47/50... Step: 16020... Loss: 0.9844... Val Loss: 1.2684\n",
            "Epoch: 47/50... Step: 16030... Loss: 1.0388... Val Loss: 1.2637\n",
            "Epoch: 47/50... Step: 16040... Loss: 0.9973... Val Loss: 1.2744\n",
            "Epoch: 47/50... Step: 16050... Loss: 0.9986... Val Loss: 1.2667\n",
            "Epoch: 47/50... Step: 16060... Loss: 0.9853... Val Loss: 1.2644\n",
            "Epoch: 47/50... Step: 16070... Loss: 0.9769... Val Loss: 1.2757\n",
            "Epoch: 47/50... Step: 16080... Loss: 0.9772... Val Loss: 1.2650\n",
            "Epoch: 47/50... Step: 16090... Loss: 0.9915... Val Loss: 1.2764\n",
            "Epoch: 47/50... Step: 16100... Loss: 1.0284... Val Loss: 1.2656\n",
            "Epoch: 47/50... Step: 16110... Loss: 0.9920... Val Loss: 1.2744\n",
            "Epoch: 47/50... Step: 16120... Loss: 1.0199... Val Loss: 1.2667\n",
            "Epoch: 47/50... Step: 16130... Loss: 0.9920... Val Loss: 1.2725\n",
            "Epoch: 47/50... Step: 16140... Loss: 0.9366... Val Loss: 1.2703\n",
            "Epoch: 47/50... Step: 16150... Loss: 0.9741... Val Loss: 1.2695\n",
            "Epoch: 47/50... Step: 16160... Loss: 0.9833... Val Loss: 1.2686\n",
            "Epoch: 47/50... Step: 16170... Loss: 0.9798... Val Loss: 1.2634\n",
            "Epoch: 47/50... Step: 16180... Loss: 0.9765... Val Loss: 1.2699\n",
            "Epoch: 47/50... Step: 16190... Loss: 1.0294... Val Loss: 1.2660\n",
            "Epoch: 47/50... Step: 16200... Loss: 0.9936... Val Loss: 1.2637\n",
            "Epoch: 47/50... Step: 16210... Loss: 0.9857... Val Loss: 1.2704\n",
            "Epoch: 47/50... Step: 16220... Loss: 0.9887... Val Loss: 1.2738\n",
            "Epoch: 47/50... Step: 16230... Loss: 1.0172... Val Loss: 1.2667\n",
            "Epoch: 47/50... Step: 16240... Loss: 1.0013... Val Loss: 1.2678\n",
            "Epoch: 47/50... Step: 16250... Loss: 0.9919... Val Loss: 1.2662\n",
            "Epoch: 47/50... Step: 16260... Loss: 0.9990... Val Loss: 1.2742\n",
            "Epoch: 47/50... Step: 16270... Loss: 1.0147... Val Loss: 1.2731\n",
            "Epoch: 47/50... Step: 16280... Loss: 1.0172... Val Loss: 1.2692\n",
            "Epoch: 47/50... Step: 16290... Loss: 0.9977... Val Loss: 1.2693\n",
            "Epoch: 47/50... Step: 16300... Loss: 1.0080... Val Loss: 1.2722\n",
            "Epoch: 47/50... Step: 16310... Loss: 0.9981... Val Loss: 1.2724\n",
            "Epoch: 47/50... Step: 16320... Loss: 0.9928... Val Loss: 1.2655\n",
            "Epoch: 47/50... Step: 16330... Loss: 1.0174... Val Loss: 1.2728\n",
            "Epoch: 47/50... Step: 16340... Loss: 1.0362... Val Loss: 1.2641\n",
            "Epoch: 47/50... Step: 16350... Loss: 0.9852... Val Loss: 1.2670\n",
            "Epoch: 48/50... Step: 16360... Loss: 0.9818... Val Loss: 1.2737\n",
            "Epoch: 48/50... Step: 16370... Loss: 1.0334... Val Loss: 1.2692\n",
            "Epoch: 48/50... Step: 16380... Loss: 0.9827... Val Loss: 1.2683\n",
            "Epoch: 48/50... Step: 16390... Loss: 0.9793... Val Loss: 1.2723\n",
            "Epoch: 48/50... Step: 16400... Loss: 1.0062... Val Loss: 1.2760\n",
            "Epoch: 48/50... Step: 16410... Loss: 0.9858... Val Loss: 1.2653\n",
            "Epoch: 48/50... Step: 16420... Loss: 1.0090... Val Loss: 1.2762\n",
            "Epoch: 48/50... Step: 16430... Loss: 1.0120... Val Loss: 1.2683\n",
            "Epoch: 48/50... Step: 16440... Loss: 0.9965... Val Loss: 1.2679\n",
            "Epoch: 48/50... Step: 16450... Loss: 0.9575... Val Loss: 1.2655\n",
            "Epoch: 48/50... Step: 16460... Loss: 0.9935... Val Loss: 1.2769\n",
            "Epoch: 48/50... Step: 16470... Loss: 0.9931... Val Loss: 1.2663\n",
            "Epoch: 48/50... Step: 16480... Loss: 0.9536... Val Loss: 1.2712\n",
            "Epoch: 48/50... Step: 16490... Loss: 1.0022... Val Loss: 1.2759\n",
            "Epoch: 48/50... Step: 16500... Loss: 0.9963... Val Loss: 1.2746\n",
            "Epoch: 48/50... Step: 16510... Loss: 0.9864... Val Loss: 1.2723\n",
            "Epoch: 48/50... Step: 16520... Loss: 1.0099... Val Loss: 1.2688\n",
            "Epoch: 48/50... Step: 16530... Loss: 0.9822... Val Loss: 1.2676\n",
            "Epoch: 48/50... Step: 16540... Loss: 0.9833... Val Loss: 1.2654\n",
            "Epoch: 48/50... Step: 16550... Loss: 1.0270... Val Loss: 1.2648\n",
            "Epoch: 48/50... Step: 16560... Loss: 0.9875... Val Loss: 1.2671\n",
            "Epoch: 48/50... Step: 16570... Loss: 0.9884... Val Loss: 1.2639\n",
            "Epoch: 48/50... Step: 16580... Loss: 1.0091... Val Loss: 1.2660\n",
            "Epoch: 48/50... Step: 16590... Loss: 0.9664... Val Loss: 1.2688\n",
            "Epoch: 48/50... Step: 16600... Loss: 1.0159... Val Loss: 1.2648\n",
            "Epoch: 48/50... Step: 16610... Loss: 1.0092... Val Loss: 1.2674\n",
            "Epoch: 48/50... Step: 16620... Loss: 1.0055... Val Loss: 1.2715\n",
            "Epoch: 48/50... Step: 16630... Loss: 1.0091... Val Loss: 1.2660\n",
            "Epoch: 48/50... Step: 16640... Loss: 0.9985... Val Loss: 1.2709\n",
            "Epoch: 48/50... Step: 16650... Loss: 0.9612... Val Loss: 1.2678\n",
            "Epoch: 48/50... Step: 16660... Loss: 1.0116... Val Loss: 1.2703\n",
            "Epoch: 48/50... Step: 16670... Loss: 1.0290... Val Loss: 1.2690\n",
            "Epoch: 48/50... Step: 16680... Loss: 1.0210... Val Loss: 1.2706\n",
            "Epoch: 48/50... Step: 16690... Loss: 1.0342... Val Loss: 1.2683\n",
            "Epoch: 48/50... Step: 16700... Loss: 0.9573... Val Loss: 1.2677\n",
            "Epoch: 49/50... Step: 16710... Loss: 1.0009... Val Loss: 1.2765\n",
            "Epoch: 49/50... Step: 16720... Loss: 0.9950... Val Loss: 1.2728\n",
            "Epoch: 49/50... Step: 16730... Loss: 1.0278... Val Loss: 1.2735\n",
            "Epoch: 49/50... Step: 16740... Loss: 1.0074... Val Loss: 1.2733\n",
            "Epoch: 49/50... Step: 16750... Loss: 0.9975... Val Loss: 1.2752\n",
            "Epoch: 49/50... Step: 16760... Loss: 0.9984... Val Loss: 1.2685\n",
            "Epoch: 49/50... Step: 16770... Loss: 0.9761... Val Loss: 1.2744\n",
            "Epoch: 49/50... Step: 16780... Loss: 1.0084... Val Loss: 1.2692\n",
            "Epoch: 49/50... Step: 16790... Loss: 0.9603... Val Loss: 1.2716\n",
            "Epoch: 49/50... Step: 16800... Loss: 0.9851... Val Loss: 1.2727\n",
            "Epoch: 49/50... Step: 16810... Loss: 0.9563... Val Loss: 1.2750\n",
            "Epoch: 49/50... Step: 16820... Loss: 0.9744... Val Loss: 1.2681\n",
            "Epoch: 49/50... Step: 16830... Loss: 0.9624... Val Loss: 1.2728\n",
            "Epoch: 49/50... Step: 16840... Loss: 1.0076... Val Loss: 1.2722\n",
            "Epoch: 49/50... Step: 16850... Loss: 0.9727... Val Loss: 1.2733\n",
            "Epoch: 49/50... Step: 16860... Loss: 0.9860... Val Loss: 1.2732\n",
            "Epoch: 49/50... Step: 16870... Loss: 0.9970... Val Loss: 1.2695\n",
            "Epoch: 49/50... Step: 16880... Loss: 0.9986... Val Loss: 1.2684\n",
            "Epoch: 49/50... Step: 16890... Loss: 0.9974... Val Loss: 1.2705\n",
            "Epoch: 49/50... Step: 16900... Loss: 1.0165... Val Loss: 1.2682\n",
            "Epoch: 49/50... Step: 16910... Loss: 0.9635... Val Loss: 1.2716\n",
            "Epoch: 49/50... Step: 16920... Loss: 1.0242... Val Loss: 1.2715\n",
            "Epoch: 49/50... Step: 16930... Loss: 1.0240... Val Loss: 1.2685\n",
            "Epoch: 49/50... Step: 16940... Loss: 1.0250... Val Loss: 1.2732\n",
            "Epoch: 49/50... Step: 16950... Loss: 1.0032... Val Loss: 1.2705\n",
            "Epoch: 49/50... Step: 16960... Loss: 0.9858... Val Loss: 1.2687\n",
            "Epoch: 49/50... Step: 16970... Loss: 0.9880... Val Loss: 1.2710\n",
            "Epoch: 49/50... Step: 16980... Loss: 0.9969... Val Loss: 1.2666\n",
            "Epoch: 49/50... Step: 16990... Loss: 1.0104... Val Loss: 1.2717\n",
            "Epoch: 49/50... Step: 17000... Loss: 0.9793... Val Loss: 1.2731\n",
            "Epoch: 49/50... Step: 17010... Loss: 1.0109... Val Loss: 1.2687\n",
            "Epoch: 49/50... Step: 17020... Loss: 0.9824... Val Loss: 1.2690\n",
            "Epoch: 49/50... Step: 17030... Loss: 1.0150... Val Loss: 1.2651\n",
            "Epoch: 49/50... Step: 17040... Loss: 1.0262... Val Loss: 1.2712\n",
            "Epoch: 49/50... Step: 17050... Loss: 0.9850... Val Loss: 1.2690\n",
            "Epoch: 50/50... Step: 17060... Loss: 0.9964... Val Loss: 1.2741\n",
            "Epoch: 50/50... Step: 17070... Loss: 0.9896... Val Loss: 1.2724\n",
            "Epoch: 50/50... Step: 17080... Loss: 0.9868... Val Loss: 1.2685\n",
            "Epoch: 50/50... Step: 17090... Loss: 0.9938... Val Loss: 1.2772\n",
            "Epoch: 50/50... Step: 17100... Loss: 1.0202... Val Loss: 1.2760\n",
            "Epoch: 50/50... Step: 17110... Loss: 0.9671... Val Loss: 1.2733\n",
            "Epoch: 50/50... Step: 17120... Loss: 0.9630... Val Loss: 1.2722\n",
            "Epoch: 50/50... Step: 17130... Loss: 0.9788... Val Loss: 1.2727\n",
            "Epoch: 50/50... Step: 17140... Loss: 0.9806... Val Loss: 1.2716\n",
            "Epoch: 50/50... Step: 17150... Loss: 0.9951... Val Loss: 1.2722\n",
            "Epoch: 50/50... Step: 17160... Loss: 0.9975... Val Loss: 1.2758\n",
            "Epoch: 50/50... Step: 17170... Loss: 1.0158... Val Loss: 1.2729\n",
            "Epoch: 50/50... Step: 17180... Loss: 0.9748... Val Loss: 1.2767\n",
            "Epoch: 50/50... Step: 17190... Loss: 0.9928... Val Loss: 1.2763\n",
            "Epoch: 50/50... Step: 17200... Loss: 0.9759... Val Loss: 1.2712\n",
            "Epoch: 50/50... Step: 17210... Loss: 0.9727... Val Loss: 1.2744\n",
            "Epoch: 50/50... Step: 17220... Loss: 0.9985... Val Loss: 1.2689\n",
            "Epoch: 50/50... Step: 17230... Loss: 0.9999... Val Loss: 1.2713\n",
            "Epoch: 50/50... Step: 17240... Loss: 0.9839... Val Loss: 1.2635\n",
            "Epoch: 50/50... Step: 17250... Loss: 0.9489... Val Loss: 1.2683\n",
            "Epoch: 50/50... Step: 17260... Loss: 0.9888... Val Loss: 1.2756\n",
            "Epoch: 50/50... Step: 17270... Loss: 1.0028... Val Loss: 1.2772\n",
            "Epoch: 50/50... Step: 17280... Loss: 1.0216... Val Loss: 1.2713\n",
            "Epoch: 50/50... Step: 17290... Loss: 1.0000... Val Loss: 1.2716\n",
            "Epoch: 50/50... Step: 17300... Loss: 1.0091... Val Loss: 1.2696\n",
            "Epoch: 50/50... Step: 17310... Loss: 1.0027... Val Loss: 1.2643\n",
            "Epoch: 50/50... Step: 17320... Loss: 1.0138... Val Loss: 1.2715\n",
            "Epoch: 50/50... Step: 17330... Loss: 0.9466... Val Loss: 1.2708\n",
            "Epoch: 50/50... Step: 17340... Loss: 1.0416... Val Loss: 1.2742\n",
            "Epoch: 50/50... Step: 17350... Loss: 0.9846... Val Loss: 1.2691\n",
            "Epoch: 50/50... Step: 17360... Loss: 0.9665... Val Loss: 1.2686\n",
            "Epoch: 50/50... Step: 17370... Loss: 0.9907... Val Loss: 1.2765\n",
            "Epoch: 50/50... Step: 17380... Loss: 1.0052... Val Loss: 1.2696\n",
            "Epoch: 50/50... Step: 17390... Loss: 1.0021... Val Loss: 1.2704\n",
            "Epoch: 50/50... Step: 17400... Loss: 1.0487... Val Loss: 1.2715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqbiCeIfkgXq"
      },
      "source": [
        "Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ildCY3kkBf"
      },
      "source": [
        "\n",
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnXPXfBHkvX9"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kq-UHEgku8m"
      },
      "source": [
        "\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoFZ8Hhjk7NI"
      },
      "source": [
        "Prime & generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0VPhoITk-ti"
      },
      "source": [
        "\n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ55LvF8lELv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9384a1-056e-4553-e965-2b705dee4acb"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna\n",
            "Alexandrovna, watching the doorway the propenty and strange and danking\n",
            "in the high coat to be annoyed that the children at the same pirtunation of\n",
            "his forgetting, the mare always did that the summer with which Anna\n",
            "had trued to go out of the room with a brown, bushes and trinding their\n",
            "chairs and brilliance of that stout, sounds, soft barrier, and the day\n",
            "before had brought the staircase wondering who do then at least the social position, half\n",
            "the steps, and the dark count of soup, the same peasant, the mother on\n",
            "her shoulders, her eyes, as it was to her husband in which he had\n",
            "been doing a good man, too, was standing at those eyes of the conversation,\n",
            "would have said against the same station, and she had to do it, and with\n",
            "the party in the sun, and a lady saw for the summer for a long thing,\n",
            "which was not an unhappiness of ate that the world handed it on the possibility\n",
            "of having throothe and more attractive. But as he shall believed\n",
            "that this present provens of what they had suffere\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSsTb1FmlJyW"
      },
      "source": [
        "Load a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk70l_uKlMOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c50f55-f4fc-4fd5-a744-bde22534de2a"
      },
      "source": [
        "\n",
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJ47OE1lQa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675edac7-fcaf-430c-d77a-d2b1e010609c"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"Leven\"))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Leven, and\n",
            "his son's sea she went into the broad room where he sand over it, a men of\n",
            "the same steps they were saying a club atmed on the right of a book, to\n",
            "summer about the deciding of a common that had come into his sees that\n",
            "they could never have spoken of it. He had all thought on him for\n",
            "this delightful, and as though to be done, and that his companions\n",
            "were considered anyone who was all the preparations of all the post,\n",
            "the mirch of his own country, who depressed a companion of his superficial\n",
            "condection. And therefore, had not called out whether he saw him.\n",
            "Though the consciousness of her heart had been turned out. She was\n",
            "delighted. She saw him what he had said to him, and saw her to show\n",
            "him a ball the creath would not be significantly, and stopped. The peasant\n",
            "was setted and shaked in first to the door, and a smile which, stirring\n",
            "a silence. She lay straight bitter than something. A clat on the trouser\n",
            "of hands, and the big, standing of a prepant departed serfun for the\n",
            "thicket, that handed the strein on its long stockings. A lot-looking\n",
            "chair, the conversation walked, and her healthy laugh were satisfaed\n",
            "or to be so trouble the crutid, they came out, at the same an official\n",
            "and attracted blood, she scowled in the salary and to bring his hands and\n",
            "went out to a long while, whispering has faced the card. \"The old man, who\n",
            "doesn't think so?\"\n",
            "\n",
            "\"Well, this!\" she said.\n",
            "\n",
            "\"I was so asking to me, are so too, I'm sure to be more,\" he said, taking\n",
            "him. \"I so distressed the same.... Why all the same to the call? Where's\n",
            "the old man! I'll tell you what's to be more than and what's\n",
            "the midwife, and walk with me, but his heart she was in a long while. He\n",
            "deed nothing but his work, and we see anything better than the possibility\n",
            "of suriously--it seems, she won't talk of them together, that you desire that\n",
            "your eyes are to make a path of their steps and\n",
            "husband, but there is anything terrible and calm for the patient. And\n",
            "how is it you cares to me, but I have not been to stay \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}